{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n"
     ]
    }
   ],
   "source": [
    "import importData as import_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "np.set_printoptions(edgeitems=10)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "uuids = [\"69508C29-2B15-4230-BE32-328E553EC63D\", \"2D2247C8-6B76-45BE-AC6B-F177A87ED4EF\", \"98006881-B2F0-4B6B-94CF-A21ABE69F21B\", \"94ABBF79-C4A7-4715-A661-94BC30627CBD\", \"DC6DB199-0AA7-441E-B896-C70A34CF7B6C\", \"2F6A5D84-2523-4EE2-BF3D-7693665B2258\", \"9026576A-3EA6-4A42-8A7A-0FD1290CF5C9\", \"BAB130C6-7D85-489A-B52E-CB0E149E585D\"]\n",
    "id = import_data.importData()\n",
    "d_final, t_final = None, None\n",
    "for uuid in uuids:\n",
    "    d, t = id.import_data(uuid, 0.2, 100, 2)\n",
    "    if d_final is None:\n",
    "        d_final, t_final = d, t\n",
    "    else:\n",
    "        # print(\"o\")\n",
    "        if len(t_final[0]) == len(t[0]):\n",
    "            d_final = np.concatenate((d_final, d), axis=0)\n",
    "            t_final = np.concatenate((t_final, t), axis=0)\n",
    "        else:\n",
    "            if len(t_final[0]) < len(t[0]):\n",
    "                difference = -len(t_final[0]) + len(t[0])\n",
    "                lengthened_t_final = []\n",
    "                for i, t_fin in enumerate(t_final):\n",
    "                    # print(np.shape(t_fin))\n",
    "                    zeros = np.zeros((difference,1))\n",
    "                    # print(np.shape(zeros))\n",
    "                    # print(t_fin)\n",
    "                    # print(zeros)\n",
    "                    lengthened_t_final.append(np.concatenate((t_fin, np.zeros((difference,1))), axis=0))\n",
    "                d_final = np.concatenate((d_final, d), axis=0)\n",
    "                t_final = np.concatenate((lengthened_t_final, t), axis=0)\n",
    "            elif len(t_final[0]) > len(t[0]):\n",
    "                difference = len(t_final[0]) - len(t[0])\n",
    "                lengthened_t= []\n",
    "                for i, t_fin in enumerate(t):\n",
    "                    # print(np.shape(t_fin))\n",
    "                    zeros = np.zeros((difference,1))\n",
    "                    # print(np.shape(zeros))\n",
    "                    # print(t_fin)\n",
    "                    # print(zeros)\n",
    "                    lengthened_t.append(np.concatenate((t_fin, np.zeros((difference,1))), axis=0))\n",
    "                d_final = np.concatenate((d_final, d), axis=0)\n",
    "                t_final = np.concatenate((t_final, lengthened_t), axis=0)\n",
    "# print(t_final)             \n",
    "t_final[t_final != 0] = 1\n",
    "\n",
    "t_final = np.sum(t_final, axis=1)\n",
    "# print(t_final)\n",
    "t_final = np.squeeze(t_final, axis=-1)\n",
    "# print(t_final)\n",
    "\n",
    "\n",
    "\n",
    "testuuid = \"99AAC7DE-4106-43AB-AA80-F5816AB4DB85\"\n",
    "d_test, t_test = id.import_data(testuuid, 0.2, 100, 2)\n",
    "t_test_original = copy.deepcopy(t_test)\n",
    "t_test[t_test != 0] = 1\n",
    "t_test = np.sum(t_test, axis=1)\n",
    "t_test = np.squeeze(t_test, axis=-1)\n",
    "# indices = list(range(len(d_test)))\n",
    "# sampled_indices = random.sample(indices, 1000)\n",
    "# sampled_indices = sorted(sampled_indices)\n",
    "# d_test, t_test = d_test[sampled_indices], t_test[sampled_indices]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Elements:  [0. 1.]\n",
      "Counts:  [3593 1090]\n"
     ]
    }
   ],
   "source": [
    "unique_t, counts = np.unique(t_final, return_counts=True)\n",
    "print(\"Unique Elements: \", unique_t)\n",
    "print(\"Counts: \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y8/qjgzy9v90vnc5zwq1l22vbvw0000gn/T/ipykernel_98460/962128042.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
      "/var/folders/y8/qjgzy9v90vnc5zwq1l22vbvw0000gn/T/ipykernel_98460/962128042.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.targets = torch.tensor(targets, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4636, 9, 200, 1)\n",
      "number of classes: 2\n",
      "number of features: 9\n",
      "Epoch 5/50, Train Loss: 0.3748, Train Accuracy: 94.50%, Val Loss: 0.4485, Val Accuracy: 88.46%, learning rate: 0.000500\n",
      "Epoch 10/50, Train Loss: 0.3705, Train Accuracy: 95.17%, Val Loss: 0.4253, Val Accuracy: 92.31%, learning rate: 0.000250\n",
      "Epoch 15/50, Train Loss: 0.3672, Train Accuracy: 95.38%, Val Loss: 0.4125, Val Accuracy: 91.94%, learning rate: 0.000125\n",
      "Epoch 20/50, Train Loss: 0.3648, Train Accuracy: 95.82%, Val Loss: 0.4218, Val Accuracy: 89.74%, learning rate: 0.000063\n",
      "Epoch 25/50, Train Loss: 0.3610, Train Accuracy: 96.18%, Val Loss: 0.4176, Val Accuracy: 91.76%, learning rate: 0.000031\n",
      "Epoch 30/50, Train Loss: 0.3600, Train Accuracy: 96.31%, Val Loss: 0.4203, Val Accuracy: 91.03%, learning rate: 0.000016\n",
      "Epoch 35/50, Train Loss: 0.3598, Train Accuracy: 96.31%, Val Loss: 0.4060, Val Accuracy: 91.94%, learning rate: 0.000008\n",
      "Epoch 40/50, Train Loss: 0.3588, Train Accuracy: 96.53%, Val Loss: 0.4157, Val Accuracy: 91.76%, learning rate: 0.000004\n",
      "Epoch 45/50, Train Loss: 0.3588, Train Accuracy: 96.68%, Val Loss: 0.4125, Val Accuracy: 92.12%, learning rate: 0.000002\n",
      "Epoch 50/50, Train Loss: 0.3583, Train Accuracy: 96.51%, Val Loss: 0.4107, Val Accuracy: 92.12%, learning rate: 0.000001\n"
     ]
    }
   ],
   "source": [
    "class SitUpDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (numpy array): Input data of shape (num_examples, num_features, num_readings, 1)\n",
    "            targets (numpy array): Target data of shape (num_examples, num_outputs, 1)\n",
    "            max_outputs (int): Maximum number of outputs per example\n",
    "        \"\"\"\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        # self.max_outputs = max_outputs\n",
    "\n",
    "        # Replace missing target times with -1 and create a mask\n",
    "        # self.masks = (self.targets > 0).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Remove the last dimension of size 1\n",
    "        x = self.inputs[idx].squeeze(-1)  # Shape: (num_features, num_readings)\n",
    "        y = self.targets[idx]  # Shape: (num_outputs)\n",
    "        # mask = self.masks[idx].squeeze(-1)  # Shape: (num_outputs)\n",
    "        return x, y#, mask\n",
    "\n",
    "class SitUpModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(SitUpModel, self).__init__()\n",
    "        \n",
    "        # Define the model layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=7, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        self.sm = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_features, num_readings)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)  # x shape: (batch_size, 128, 1)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 128)\n",
    "        x = self.fc(x)  # Output shape: (batch_size, num_classes)\n",
    "        x = self.sm(x)\n",
    "        return x\n",
    "    \n",
    "def mse_loss(predictions, targets):\n",
    "    # Compute MSE loss only over valid targets\n",
    "    diff = (predictions - targets)\n",
    "    loss = (diff ** 2).sum() / len(predictions)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "# Set device\n",
    "if   torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "x_test = torch.tensor(d_test, dtype=torch.float32).to(device)\n",
    "test_dataset = SitUpDataset(x_test, t_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Split Data into validation and training sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(d_final, t_final, test_size=0.01)\n",
    "print(np.shape(X_train))\n",
    "\n",
    "# Gather information on the dataset\n",
    "num_classes = int(np.max(Y_train, axis=0) + 1)\n",
    "num_features = len(X_train[0])\n",
    "print(f\"number of classes: {num_classes}\")\n",
    "print(f\"number of features: {num_features}\")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "Y_val = torch.tensor(Y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = SitUpDataset(X_train, Y_train)\n",
    "val_dataset = SitUpDataset(X_val, Y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Initialize the model, optimizer, and scheduler with new learning rate\n",
    "model = SitUpModel(num_features=num_features, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50  # Initial number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # Load the batch and conver to the correct datatypes\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.long().to(device)\n",
    "\n",
    "        # Calculate batch outputs and loss\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Optimise for the next step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate training stats\n",
    "        total_train_loss += loss.item() * x_batch.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_eval_loss = 0.0\n",
    "    total_eval_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            # Load the batch and conver to the correct datatypes\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.long().to(device)\n",
    "\n",
    "            # Calculate batch outputs and loss\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Calculate training stats\n",
    "            total_eval_loss += loss.item() * x_batch.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_eval_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_train_loss = total_train_loss / len(train_dataset)\n",
    "    train_accuracy = 100* total_train_correct / len(train_dataset)\n",
    "    avg_val_loss = total_eval_loss / len(test_dataset)\n",
    "    val_accuracy = 100* total_eval_correct / len(test_dataset)\n",
    "\n",
    "    if (epoch +1)% 5 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# Save the trained model and optimizer state\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'loss': loss.item(),\n",
    "}, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/var/folders/y8/qjgzy9v90vnc5zwq1l22vbvw0000gn/T/ipykernel_98460/962128042.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
      "/opt/anaconda3/envs/DS_Torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [0.986584   0.01341602] 0 [0.]\n",
      "✅ [0.98611754 0.01388246] 0 [0.]\n",
      "✅ [0.9894449  0.01055506] 0 [0.]\n",
      "✅ [0.984611   0.01538904] 0 [0.]\n",
      "✅ [0.96502966 0.03497033] 0 [0.]\n",
      "❌ [0.89380115 0.10619882] 1 [1.949934]\n",
      "❌ [0.59882873 0.40117127] 1 [1.749934]\n",
      "✅ [0.1592231 0.8407769] 1 [1.549934]\n",
      "✅ [0.03039449 0.9696055 ] 1 [1.349934]\n",
      "✅ [0.01066818 0.98933184] 1 [1.149934]\n",
      "✅ [0.01414452 0.9858555 ] 1 [0.949934]\n",
      "✅ [0.01518527 0.98481476] 1 [0.749934]\n",
      "✅ [0.01194578 0.9880542 ] 1 [0.549934]\n",
      "✅ [0.01794438 0.9820556 ] 1 [0.349934]\n",
      "✅ [0.03752715 0.9624728 ] 1 [0.14993401]\n",
      "❌ [0.11384608 0.88615394] 0 [0.]\n",
      "❌ [0.37296373 0.6270363 ] 0 [0.]\n",
      "✅ [0.7132903  0.28670976] 0 [0.]\n",
      "✅ [0.90723324 0.09276675] 0 [0.]\n",
      "✅ [0.9690494  0.03095059] 0 [0.]\n",
      "✅ [0.9865975  0.01340253] 0 [0.]\n",
      "✅ [0.9929584  0.00704155] 0 [0.]\n",
      "✅ [0.99614394 0.003856  ] 0 [0.]\n",
      "✅ [0.99748635 0.00251359] 0 [0.]\n",
      "✅ [0.99747247 0.00252748] 0 [0.]\n",
      "✅ [0.99769694 0.00230305] 0 [0.]\n",
      "✅ [0.99336195 0.00663807] 0 [0.]\n",
      "✅ [0.9820811  0.01791885] 0 [0.]\n",
      "✅ [0.945192   0.05480808] 0 [0.]\n",
      "❌ [0.72900486 0.2709951 ] 1 [1.9699576]\n",
      "✅ [0.29758093 0.70241904] 1 [1.7699575]\n",
      "✅ [0.05417825 0.9458217 ] 1 [1.5699575]\n",
      "✅ [0.02380431 0.97619575] 1 [1.3699576]\n",
      "✅ [0.01588555 0.9841144 ] 1 [1.1699575]\n",
      "✅ [0.0125677  0.98743236] 1 [0.96995753]\n",
      "✅ [0.01559592 0.984404  ] 1 [0.76995754]\n",
      "✅ [0.01920494 0.9807951 ] 1 [0.56995755]\n",
      "✅ [0.02784866 0.9721513 ] 1 [0.36995754]\n",
      "✅ [0.08214047 0.9178595 ] 1 [0.16995755]\n",
      "❌ [0.2644145 0.7355856] 0 [0.]\n",
      "✅ [0.64826214 0.3517379 ] 0 [0.]\n",
      "✅ [0.8536462 0.1463538] 0 [0.]\n",
      "✅ [0.93302983 0.06697024] 0 [0.]\n",
      "✅ [0.9637593  0.03624072] 0 [0.]\n",
      "✅ [0.9782963  0.02170374] 0 [0.]\n",
      "✅ [0.985518   0.01448202] 0 [0.]\n",
      "✅ [0.98541415 0.01458583] 0 [0.]\n",
      "✅ [0.98291874 0.01708129] 0 [0.]\n",
      "✅ [0.9693266  0.03067334] 0 [0.]\n",
      "✅ [0.95878077 0.04121925] 0 [0.]\n",
      "❌ [0.8291598  0.17084025] 1 [1.9600236]\n",
      "✅ [0.27760866 0.72239137] 1 [1.7600236]\n",
      "✅ [0.03920978 0.9607903 ] 1 [1.5600237]\n",
      "✅ [0.01900369 0.98099625] 1 [1.3600236]\n",
      "✅ [0.01170984 0.9882902 ] 1 [1.1600237]\n",
      "✅ [0.00893441 0.99106556] 1 [0.96002364]\n",
      "✅ [0.00778103 0.992219  ] 1 [0.76002365]\n",
      "✅ [0.01102324 0.9889768 ] 1 [0.56002367]\n",
      "✅ [0.02020772 0.97979224] 1 [0.36002365]\n",
      "✅ [0.07234188 0.9276581 ] 1 [0.16002364]\n",
      "❌ [0.27563298 0.7243671 ] 0 [0.]\n",
      "✅ [0.7161519 0.2838481] 0 [0.]\n",
      "✅ [0.913491   0.08650904] 0 [0.]\n",
      "✅ [0.9654731  0.03452689] 0 [0.]\n",
      "✅ [0.981156   0.01884404] 0 [0.]\n",
      "✅ [0.98895395 0.01104601] 0 [0.]\n",
      "✅ [0.99255556 0.0074444 ] 0 [0.]\n",
      "✅ [0.9948487  0.00515125] 0 [0.]\n",
      "✅ [0.99281174 0.00718823] 0 [0.]\n",
      "✅ [0.9890956  0.01090433] 0 [0.]\n",
      "✅ [0.9462956  0.05370434] 0 [0.]\n",
      "❌ [0.5930042  0.40699583] 1 [1.8699946]\n",
      "✅ [0.15033267 0.8496674 ] 1 [1.6699947]\n",
      "✅ [0.03599376 0.96400625] 1 [1.4699947]\n",
      "✅ [0.02153167 0.97846836] 1 [1.2699947]\n",
      "✅ [0.01546822 0.9845318 ] 1 [1.0699947]\n",
      "✅ [0.0120949 0.987905 ] 1 [0.8699947]\n",
      "✅ [0.01356708 0.98643297] 1 [0.6699947]\n",
      "✅ [0.02236309 0.97763693] 1 [0.4699947]\n",
      "✅ [0.07882562 0.92117435] 1 [0.26999468]\n",
      "✅ [0.38839203 0.61160797] 1 [0.06999469]\n",
      "✅ [0.7973437  0.20265633] 0 [0.]\n",
      "✅ [0.9367574  0.06324258] 0 [0.]\n",
      "✅ [0.973462   0.02653799] 0 [0.]\n",
      "✅ [0.983022 0.016978] 0 [0.]\n",
      "✅ [0.98684454 0.01315544] 0 [0.]\n",
      "✅ [0.9888391  0.01116085] 0 [0.]\n",
      "✅ [0.98894113 0.01105886] 0 [0.]\n",
      "✅ [0.9856997  0.01430035] 0 [0.]\n",
      "✅ [0.9673794  0.03262059] 0 [0.]\n",
      "✅ [0.8063039 0.1936961] 0 [0.]\n",
      "✅ [0.27557644 0.7244236 ] 1 [1.9399483]\n",
      "✅ [0.07537974 0.9246203 ] 1 [1.7399484]\n",
      "✅ [0.04276525 0.95723474] 1 [1.5399483]\n",
      "✅ [0.02929084 0.9707091 ] 1 [1.3399484]\n",
      "✅ [0.02256491 0.9774351 ] 1 [1.1399484]\n",
      "✅ [0.02007961 0.97992045] 1 [0.9399484]\n",
      "✅ [0.0250586 0.9749414] 1 [0.7399484]\n",
      "✅ [0.03180692 0.9681931 ] 1 [0.53994834]\n",
      "✅ [0.09847835 0.9015216 ] 1 [0.33994836]\n",
      "✅ [0.36961076 0.6303892 ] 1 [0.13994837]\n",
      "✅ [0.72397405 0.27602595] 0 [0.]\n",
      "✅ [0.9050999  0.09490005] 0 [0.]\n",
      "✅ [0.95461327 0.04538672] 0 [0.]\n",
      "✅ [0.96786165 0.03213829] 0 [0.]\n",
      "✅ [0.9741194  0.02588052] 0 [0.]\n",
      "✅ [0.9726782  0.02732178] 0 [0.]\n",
      "✅ [0.9641062  0.03589386] 0 [0.]\n",
      "✅ [0.8956087 0.1043913] 0 [0.]\n",
      "✅ [0.67339516 0.32660484] 0 [0.]\n",
      "✅ [0.24876496 0.751235  ] 1 [1.8700032]\n",
      "✅ [0.06030668 0.9396933 ] 1 [1.6700032]\n",
      "✅ [0.03250246 0.9674975 ] 1 [1.4700032]\n",
      "✅ [0.02843601 0.971564  ] 1 [1.2700032]\n",
      "✅ [0.02367323 0.9763267 ] 1 [1.0700033]\n",
      "✅ [0.02399412 0.9760059 ] 1 [0.8700032]\n",
      "✅ [0.02788386 0.9721162 ] 1 [0.67000324]\n",
      "✅ [0.04325504 0.95674497] 1 [0.47000322]\n",
      "✅ [0.16110037 0.8388996 ] 1 [0.27000323]\n",
      "❌ [0.60631615 0.39368382] 1 [0.07000323]\n",
      "✅ [0.8722996 0.1277004] 0 [0.]\n",
      "✅ [0.9512425 0.0487575] 0 [0.]\n",
      "✅ [0.96718365 0.03281634] 0 [0.]\n",
      "✅ [0.977075   0.02292503] 0 [0.]\n",
      "✅ [0.9807663  0.01923372] 0 [0.]\n",
      "✅ [0.98557776 0.01442224] 0 [0.]\n",
      "✅ [0.9859678  0.01403218] 0 [0.]\n",
      "✅ [0.97682494 0.02317504] 0 [0.]\n",
      "✅ [0.9615895  0.03841047] 0 [0.]\n",
      "✅ [0.77778435 0.22221567] 0 [0.]\n",
      "✅ [0.34433863 0.6556614 ] 1 [1.9599826]\n",
      "✅ [0.04659412 0.95340586] 1 [1.7599826]\n",
      "✅ [0.01784093 0.9821591 ] 1 [1.5599827]\n",
      "✅ [0.01194265 0.9880574 ] 1 [1.3599826]\n",
      "✅ [0.01068604 0.98931396] 1 [1.1599827]\n",
      "✅ [0.00901306 0.9909869 ] 1 [0.95998263]\n",
      "✅ [0.00982484 0.99017525] 1 [0.75998265]\n",
      "✅ [0.01905723 0.9809428 ] 1 [0.55998266]\n",
      "✅ [0.04281824 0.95718175] 1 [0.35998264]\n",
      "✅ [0.22997238 0.77002764] 1 [0.15998264]\n",
      "✅ [0.64648426 0.35351568] 0 [0.]\n",
      "✅ [0.8805734  0.11942658] 0 [0.]\n",
      "✅ [0.96561885 0.03438119] 0 [0.]\n",
      "✅ [0.9863272  0.01367279] 0 [0.]\n",
      "✅ [0.9932147  0.00678525] 0 [0.]\n",
      "✅ [0.9962521  0.00374796] 0 [0.]\n",
      "✅ [0.997591   0.00240898] 0 [0.]\n",
      "✅ [0.99780434 0.00219561] 0 [0.]\n",
      "✅ [0.997408   0.00259201] 0 [0.]\n",
      "✅ [0.9964413  0.00355863] 0 [0.]\n",
      "✅ [0.99087954 0.0091204 ] 0 [0.]\n",
      "✅ [0.971626   0.02837394] 0 [0.]\n",
      "✅ [0.8456221 0.1543779] 0 [0.]\n",
      "❌ [0.31603122 0.68396884] 0 [0.]\n",
      "✅ [0.05998515 0.94001484] 1 [1.8899958]\n",
      "✅ [0.02037051 0.97962946] 1 [1.6899958]\n",
      "✅ [0.01139229 0.9886077 ] 1 [1.4899957]\n",
      "✅ [0.00976423 0.99023575] 1 [1.2899958]\n",
      "✅ [0.00840766 0.9915923 ] 1 [1.0899957]\n",
      "✅ [0.01360286 0.9863971 ] 1 [0.88999575]\n",
      "✅ [0.02954859 0.9704514 ] 1 [0.68999577]\n",
      "✅ [0.1233681 0.8766319] 1 [0.48999578]\n",
      "❌ [0.5738235  0.42617646] 1 [0.28999576]\n",
      "❌ [0.89710534 0.10289462] 1 [0.08999576]\n",
      "✅ [0.96757746 0.03242255] 0 [0.]\n",
      "✅ [0.98390955 0.01609046] 0 [0.]\n",
      "✅ [0.98956084 0.01043917] 0 [0.]\n",
      "✅ [0.99297684 0.00702308] 0 [0.]\n",
      "✅ [0.99161094 0.00838902] 0 [0.]\n",
      "✅ [0.9879319  0.01206812] 0 [0.]\n",
      "✅ [0.961746   0.03825408] 0 [0.]\n",
      "✅ [0.7422716  0.25772837] 0 [0.]\n",
      "✅ [0.15364878 0.84635127] 1 [1.929999]\n",
      "✅ [0.03104999 0.96895   ] 1 [1.729999]\n",
      "✅ [0.021564   0.97843593] 1 [1.529999]\n",
      "✅ [0.01324377 0.9867562 ] 1 [1.329999]\n",
      "✅ [0.00849391 0.99150604] 1 [1.1299989]\n",
      "✅ [0.00862145 0.99137855] 1 [0.929999]\n",
      "✅ [0.01049704 0.9895029 ] 1 [0.72999895]\n",
      "✅ [0.02300529 0.97699463] 1 [0.52999896]\n",
      "✅ [0.06561168 0.93438834] 1 [0.32999897]\n",
      "✅ [0.23570472 0.7642952 ] 1 [0.12999897]\n",
      "✅ [0.675921   0.32407898] 0 [0.]\n",
      "✅ [0.9069249  0.09307513] 0 [0.]\n",
      "✅ [0.9702256  0.02977448] 0 [0.]\n",
      "✅ [0.98470867 0.01529131] 0 [0.]\n",
      "✅ [0.9909157  0.00908428] 0 [0.]\n",
      "✅ [0.9941567  0.00584331] 0 [0.]\n",
      "✅ [0.99456394 0.00543606] 0 [0.]\n",
      "✅ [0.9945023  0.00549774] 0 [0.]\n",
      "✅ [0.9887932  0.01120682] 0 [0.]\n",
      "✅ [0.9669982  0.03300177] 0 [0.]\n",
      "✅ [0.8159093  0.18409069] 0 [0.]\n",
      "✅ [0.21636909 0.7836309 ] 1 [1.9399365]\n",
      "✅ [0.04169485 0.9583051 ] 1 [1.7399366]\n",
      "✅ [0.02049795 0.979502  ] 1 [1.5399365]\n",
      "✅ [0.01300595 0.9869941 ] 1 [1.3399365]\n",
      "✅ [0.01079175 0.9892083 ] 1 [1.1399366]\n",
      "✅ [0.01091043 0.98908955] 1 [0.9399365]\n",
      "✅ [0.01259775 0.98740226] 1 [0.73993653]\n",
      "✅ [0.01958548 0.9804145 ] 1 [0.53993654]\n",
      "✅ [0.03646853 0.96353143] 1 [0.33993655]\n",
      "✅ [0.12604116 0.8739588 ] 1 [0.13993654]\n",
      "❌ [0.4277267 0.5722733] 0 [0.]\n",
      "✅ [0.8251467  0.17485335] 0 [0.]\n",
      "✅ [0.96075803 0.03924196] 0 [0.]\n",
      "✅ [0.98469216 0.01530783] 0 [0.]\n",
      "✅ [0.99095964 0.00904033] 0 [0.]\n",
      "✅ [0.9940507  0.00594935] 0 [0.]\n",
      "✅ [0.9953153  0.00468474] 0 [0.]\n",
      "✅ [0.99386483 0.00613511] 0 [0.]\n",
      "✅ [0.99091095 0.00908904] 0 [0.]\n",
      "✅ [0.9768411  0.02315898] 0 [0.]\n",
      "✅ [0.8785444  0.12145562] 0 [0.]\n",
      "❌ [0.4055018 0.5944982] 0 [0.]\n",
      "✅ [0.06137494 0.9386251 ] 1 [1.8099539]\n",
      "✅ [0.01995821 0.9800418 ] 1 [1.6099539]\n",
      "✅ [0.01261468 0.9873853 ] 1 [1.409954]\n",
      "✅ [0.00959874 0.9904013 ] 1 [1.2099539]\n",
      "✅ [0.00798824 0.9920117 ] 1 [1.009954]\n",
      "✅ [0.00886796 0.9911321 ] 1 [0.8099539]\n",
      "✅ [0.01378473 0.9862153 ] 1 [0.60995394]\n",
      "✅ [0.03397705 0.96602297] 1 [0.40995392]\n",
      "✅ [0.16041493 0.839585  ] 1 [0.20995393]\n",
      "❌ [0.5054089  0.49459115] 1 [0.00995393]\n",
      "✅ [0.82919496 0.17080505] 0 [0.]\n",
      "✅ [0.9510196  0.04898043] 0 [0.]\n",
      "✅ [0.9775329  0.02246711] 0 [0.]\n",
      "✅ [0.9883334  0.01166666] 0 [0.]\n",
      "✅ [0.9929287  0.00707131] 0 [0.]\n",
      "✅ [0.9951416  0.00485836] 0 [0.]\n",
      "✅ [0.99428314 0.0057168 ] 0 [0.]\n",
      "✅ [0.9936526  0.00634745] 0 [0.]\n",
      "✅ [0.985021   0.01497897] 0 [0.]\n",
      "✅ [0.94856477 0.05143522] 0 [0.]\n",
      "✅ [0.72652847 0.27347153] 0 [0.]\n",
      "✅ [0.2023072  0.79769284] 1 [1.9600036]\n",
      "✅ [0.05850979 0.9414902 ] 1 [1.7600037]\n",
      "✅ [0.02717758 0.9728224 ] 1 [1.5600036]\n",
      "✅ [0.01939015 0.98060983] 1 [1.3600037]\n",
      "✅ [0.01485998 0.98514   ] 1 [1.1600037]\n",
      "✅ [0.01425012 0.98574984] 1 [0.9600037]\n",
      "✅ [0.01907133 0.9809287 ] 1 [0.7600037]\n",
      "✅ [0.03183796 0.96816206] 1 [0.56000364]\n",
      "✅ [0.08752362 0.91247636] 1 [0.36000365]\n",
      "✅ [0.38028142 0.61971855] 1 [0.16000366]\n",
      "✅ [0.79800326 0.20199676] 0 [0.]\n",
      "✅ [0.95382416 0.04617586] 0 [0.]\n",
      "✅ [0.98457354 0.01542648] 0 [0.]\n",
      "✅ [0.99106956 0.00893041] 0 [0.]\n",
      "✅ [0.9938399  0.00616013] 0 [0.]\n",
      "✅ [0.9949568  0.00504317] 0 [0.]\n",
      "✅ [0.9962298  0.00377011] 0 [0.]\n",
      "✅ [0.99551755 0.00448247] 0 [0.]\n",
      "✅ [0.995365   0.00463498] 0 [0.]\n",
      "✅ [0.9914253  0.00857466] 0 [0.]\n",
      "✅ [0.9701399  0.02986006] 0 [0.]\n",
      "✅ [0.83997697 0.16002303] 0 [0.]\n",
      "❌ [0.29915977 0.70084023] 0 [0.]\n",
      "✅ [0.05248837 0.9475117 ] 1 [1.8000019]\n",
      "✅ [0.02211878 0.9778812 ] 1 [1.6000018]\n",
      "✅ [0.01545054 0.9845495 ] 1 [1.4000018]\n",
      "✅ [0.01096683 0.9890331 ] 1 [1.2000018]\n",
      "✅ [0.00890051 0.99109954] 1 [1.0000018]\n",
      "✅ [0.00934321 0.99065673] 1 [0.8000018]\n",
      "✅ [0.01451433 0.9854857 ] 1 [0.6000018]\n",
      "✅ [0.03197847 0.9680215 ] 1 [0.40000182]\n",
      "✅ [0.10773089 0.8922691 ] 1 [0.2000018]\n",
      "✅ [0.39580175 0.6041983 ] 1 [1.8119812e-06]\n",
      "✅ [0.82448274 0.17551728] 0 [0.]\n",
      "✅ [0.9453786  0.05462136] 0 [0.]\n",
      "✅ [0.9785429  0.02145706] 0 [0.]\n",
      "✅ [0.9884333  0.01156669] 0 [0.]\n",
      "✅ [0.9922699  0.00773006] 0 [0.]\n",
      "✅ [0.9941883  0.00581173] 0 [0.]\n",
      "✅ [0.9912101  0.00878991] 0 [0.]\n",
      "✅ [0.9870179  0.01298209] 0 [0.]\n",
      "✅ [0.9512399  0.04876011] 0 [0.]\n",
      "✅ [0.70623356 0.29376644] 0 [0.]\n",
      "✅ [0.17561778 0.82438225] 1 [1.8999879]\n",
      "✅ [0.04334735 0.9566527 ] 1 [1.6999879]\n",
      "✅ [0.01895019 0.98104984] 1 [1.4999878]\n",
      "✅ [0.01287694 0.987123  ] 1 [1.2999879]\n",
      "✅ [0.01043821 0.98956174] 1 [1.0999879]\n",
      "✅ [0.01273938 0.9872606 ] 1 [0.8999879]\n",
      "✅ [0.02249331 0.97750676] 1 [0.6999879]\n",
      "✅ [0.05902239 0.9409776 ] 1 [0.4999879]\n",
      "✅ [0.32104012 0.6789599 ] 1 [0.29998788]\n",
      "❌ [0.80835277 0.19164722] 1 [0.09998789]\n",
      "✅ [0.96617436 0.03382564] 0 [0.]\n",
      "✅ [0.99216837 0.00783162] 0 [0.]\n",
      "✅ [0.9968957  0.00310426] 0 [0.]\n",
      "✅ [0.9985312  0.00146877] 0 [0.]\n",
      "✅ [9.9923503e-01 7.6500978e-04] 0 [0.]\n",
      "✅ [9.994584e-01 5.416165e-04] 0 [0.]\n",
      "✅ [9.9936634e-01 6.3365028e-04] 0 [0.]\n",
      "✅ [9.9917513e-01 8.2486204e-04] 0 [0.]\n",
      "✅ [0.99831796 0.00168208] 0 [0.]\n",
      "✅ [0.9967692  0.00323078] 0 [0.]\n",
      "✅ [0.99446106 0.00553891] 0 [0.]\n",
      "✅ [0.9911488  0.00885111] 0 [0.]\n",
      "✅ [0.9861822  0.01381779] 0 [0.]\n",
      "✅ [0.9701269  0.02987304] 0 [0.]\n",
      "✅ [0.8310333  0.16896667] 0 [0.]\n",
      "✅ [0.28287813 0.7171219 ] 1 [1.8499553]\n",
      "✅ [0.04586896 0.954131  ] 1 [1.6499553]\n",
      "✅ [0.02168145 0.9783186 ] 1 [1.4499552]\n",
      "✅ [0.01324668 0.9867533 ] 1 [1.2499553]\n",
      "✅ [0.00949696 0.9905031 ] 1 [1.0499552]\n",
      "✅ [0.00831163 0.99168843] 1 [0.84995526]\n",
      "✅ [0.01050528 0.98949474] 1 [0.6499553]\n",
      "✅ [0.02956863 0.9704314 ] 1 [0.44995528]\n",
      "✅ [0.13039333 0.8696067 ] 1 [0.24995527]\n",
      "❌ [0.5290154  0.47098455] 1 [0.04995527]\n",
      "✅ [0.83341336 0.16658661] 0 [0.]\n",
      "✅ [0.9387145 0.0612855] 0 [0.]\n",
      "✅ [0.9751992  0.02480079] 0 [0.]\n",
      "✅ [0.98896676 0.01103318] 0 [0.]\n",
      "✅ [0.99331915 0.00668086] 0 [0.]\n",
      "✅ [0.99572694 0.00427306] 0 [0.]\n",
      "✅ [0.9950799  0.00492016] 0 [0.]\n",
      "✅ [0.9938666  0.00613333] 0 [0.]\n",
      "✅ [0.9850211  0.01497887] 0 [0.]\n",
      "❌ [0.92414844 0.07585153] 1 [1.9499453]\n",
      "❌ [0.6473294  0.35267058] 1 [1.7499454]\n",
      "✅ [0.13628797 0.863712  ] 1 [1.5499454]\n",
      "✅ [0.04384311 0.95615685] 1 [1.3499453]\n",
      "✅ [0.02431575 0.9756842 ] 1 [1.1499454]\n",
      "✅ [0.01580524 0.98419476] 1 [0.94994533]\n",
      "✅ [0.01044474 0.98955524] 1 [0.74994534]\n",
      "✅ [0.00890264 0.99109733] 1 [0.54994535]\n",
      "✅ [0.01190467 0.9880953 ] 1 [0.34994537]\n",
      "✅ [0.02164941 0.9783506 ] 1 [0.14994535]\n",
      "❌ [0.08082116 0.91917884] 0 [0.]\n",
      "❌ [0.32229093 0.67770904] 0 [0.]\n",
      "✅ [0.7053121  0.29468796] 0 [0.]\n",
      "✅ [0.893043   0.10695702] 0 [0.]\n",
      "✅ [0.95996535 0.04003463] 0 [0.]\n",
      "✅ [0.97844374 0.02155631] 0 [0.]\n",
      "✅ [0.9867654  0.01323465] 0 [0.]\n",
      "✅ [0.9929155  0.00708446] 0 [0.]\n",
      "✅ [0.9943381 0.0056619] 0 [0.]\n",
      "✅ [0.99307656 0.00692351] 0 [0.]\n",
      "✅ [0.9850181  0.01498192] 0 [0.]\n",
      "✅ [0.9231801  0.07681985] 0 [0.]\n",
      "❌ [0.57819164 0.4218084 ] 1 [1.9699955]\n",
      "✅ [0.13529123 0.8647088 ] 1 [1.7699955]\n",
      "✅ [0.04139118 0.95860887] 1 [1.5699955]\n",
      "✅ [0.01676286 0.9832372 ] 1 [1.3699955]\n",
      "✅ [0.0116398  0.98836017] 1 [1.1699955]\n",
      "✅ [0.00925295 0.9907471 ] 1 [0.9699955]\n",
      "✅ [0.00816486 0.9918351 ] 1 [0.7699955]\n",
      "✅ [0.00828811 0.99171185] 1 [0.5699955]\n",
      "✅ [0.01387862 0.98612136] 1 [0.3699955]\n",
      "✅ [0.05586801 0.944132  ] 1 [0.1699955]\n",
      "❌ [0.17313015 0.82686985] 0 [0.]\n",
      "❌ [0.46231425 0.5376857 ] 0 [0.]\n",
      "✅ [0.8112062  0.18879376] 0 [0.]\n",
      "✅ [0.9611484 0.0388516] 0 [0.]\n",
      "✅ [0.9850108  0.01498925] 0 [0.]\n",
      "✅ [0.99244267 0.00755736] 0 [0.]\n",
      "✅ [0.99543977 0.00456019] 0 [0.]\n",
      "✅ [0.99682814 0.00317183] 0 [0.]\n",
      "✅ [0.9942836  0.00571633] 0 [0.]\n",
      "✅ [0.99233466 0.00766535] 0 [0.]\n",
      "✅ [0.98350835 0.01649167] 0 [0.]\n",
      "✅ [0.9542286  0.04577144] 0 [0.]\n",
      "❌ [0.7664784  0.23352157] 1 [1.8400348]\n",
      "✅ [0.29102707 0.70897293] 1 [1.6400348]\n",
      "✅ [0.10928535 0.89071465] 1 [1.4400349]\n",
      "✅ [0.05196778 0.9480322 ] 1 [1.2400348]\n",
      "✅ [0.02630147 0.97369856] 1 [1.0400348]\n",
      "✅ [0.01254148 0.98745847] 1 [0.84003484]\n",
      "✅ [0.00623003 0.99376994] 1 [0.6400348]\n",
      "✅ [0.00455441 0.9954456 ] 1 [0.4400348]\n",
      "✅ [0.00489511 0.9951049 ] 1 [0.24003482]\n",
      "✅ [0.00807473 0.9919253 ] 1 [0.04003482]\n",
      "❌ [0.02603577 0.9739643 ] 0 [0.]\n",
      "❌ [0.12685387 0.8731461 ] 0 [0.]\n",
      "❌ [0.35241118 0.64758885] 0 [0.]\n",
      "✅ [0.6776451 0.3223549] 0 [0.]\n",
      "✅ [0.9003468  0.09965318] 0 [0.]\n",
      "✅ [0.9734422  0.02655781] 0 [0.]\n",
      "✅ [0.9920632  0.00793676] 0 [0.]\n",
      "✅ [0.9956365 0.0043635] 0 [0.]\n",
      "✅ [0.9953243  0.00467569] 0 [0.]\n",
      "✅ [0.9866477  0.01335228] 0 [0.]\n",
      "✅ [0.9159411  0.08405886] 0 [0.]\n",
      "✅ [0.36348897 0.636511  ] 1 [1.8000025]\n",
      "✅ [0.05371915 0.9462809 ] 1 [1.6000025]\n",
      "✅ [0.0311517 0.9688483] 1 [1.4000025]\n",
      "✅ [0.0216258  0.97837424] 1 [1.2000024]\n",
      "✅ [0.01436292 0.985637  ] 1 [1.0000025]\n",
      "✅ [0.01097839 0.9890216 ] 1 [0.80000246]\n",
      "✅ [0.00916925 0.9908308 ] 1 [0.60000247]\n",
      "✅ [0.01533365 0.98466635] 1 [0.40000248]\n",
      "✅ [0.04121684 0.9587832 ] 1 [0.20000248]\n",
      "✅ [0.16110352 0.83889645] 1 [2.4795531e-06]\n",
      "❌ [0.48673427 0.5132657 ] 0 [0.]\n",
      "✅ [0.8116388  0.18836127] 0 [0.]\n",
      "✅ [0.92773956 0.07226046] 0 [0.]\n",
      "✅ [0.97480464 0.0251954 ] 0 [0.]\n",
      "✅ [0.99032277 0.00967728] 0 [0.]\n",
      "✅ [0.99619436 0.00380566] 0 [0.]\n",
      "✅ [0.9963373  0.00366274] 0 [0.]\n",
      "✅ [0.9957897  0.00421037] 0 [0.]\n",
      "✅ [0.98948634 0.01051364] 0 [0.]\n",
      "✅ [0.950804   0.04919595] 0 [0.]\n",
      "✅ [0.6864021 0.3135979] 0 [0.]\n",
      "✅ [0.15895039 0.8410496 ] 1 [1.9399996]\n",
      "✅ [0.03840382 0.96159613] 1 [1.7399995]\n",
      "✅ [0.01544692 0.9845531 ] 1 [1.5399995]\n",
      "✅ [0.00869438 0.99130565] 1 [1.3399996]\n",
      "✅ [0.00719553 0.99280447] 1 [1.1399995]\n",
      "✅ [0.00702804 0.99297196] 1 [0.9399995]\n",
      "✅ [0.01152083 0.9884792 ] 1 [0.73999953]\n",
      "✅ [0.0227104  0.97728956] 1 [0.53999954]\n",
      "✅ [0.09085756 0.90914243] 1 [0.33999953]\n",
      "✅ [0.38852918 0.6114708 ] 1 [0.13999954]\n",
      "✅ [0.7515227  0.24847727] 0 [0.]\n",
      "✅ [0.92876685 0.07123323] 0 [0.]\n",
      "✅ [0.9710707  0.02892924] 0 [0.]\n",
      "✅ [0.98548037 0.0145196 ] 0 [0.]\n",
      "✅ [0.9912896  0.00871041] 0 [0.]\n",
      "✅ [0.99447143 0.00552854] 0 [0.]\n",
      "✅ [0.9946365  0.00536355] 0 [0.]\n",
      "✅ [0.98809904 0.01190097] 0 [0.]\n",
      "✅ [0.9562626  0.04373739] 0 [0.]\n",
      "✅ [0.7726689  0.22733107] 0 [0.]\n",
      "✅ [0.27156225 0.7284378 ] 1 [1.91995]\n",
      "✅ [0.04310972 0.9568902 ] 1 [1.71995]\n",
      "✅ [0.01662586 0.9833741 ] 1 [1.5199499]\n",
      "✅ [0.00987117 0.9901288 ] 1 [1.31995]\n",
      "✅ [0.00892308 0.99107695] 1 [1.1199499]\n",
      "✅ [0.00737887 0.99262106] 1 [0.91994995]\n",
      "✅ [0.00685896 0.9931411 ] 1 [0.71994996]\n",
      "✅ [0.01221077 0.98778915] 1 [0.51995]\n",
      "✅ [0.04238325 0.95761675] 1 [0.31994995]\n",
      "✅ [0.18412931 0.8158707 ] 1 [0.11994996]\n",
      "✅ [0.61083114 0.3891688 ] 0 [0.]\n",
      "✅ [0.8958594  0.10414064] 0 [0.]\n",
      "✅ [0.9800181  0.01998191] 0 [0.]\n",
      "✅ [0.991549   0.00845092] 0 [0.]\n",
      "✅ [0.99503905 0.00496091] 0 [0.]\n",
      "✅ [0.9965479  0.00345213] 0 [0.]\n",
      "✅ [0.9965784  0.00342155] 0 [0.]\n",
      "✅ [0.99629694 0.00370304] 0 [0.]\n",
      "✅ [0.9938746  0.00612533] 0 [0.]\n",
      "✅ [0.98494476 0.01505522] 0 [0.]\n",
      "✅ [0.9510313  0.04896862] 0 [0.]\n",
      "❌ [0.7362758  0.26372427] 1 [1.8699379]\n",
      "✅ [0.18112056 0.8188795 ] 1 [1.6699378]\n",
      "✅ [0.03746015 0.96253985] 1 [1.4699379]\n",
      "✅ [0.01664745 0.98335254] 1 [1.2699379]\n",
      "✅ [0.01345644 0.9865436 ] 1 [1.069938]\n",
      "✅ [0.01164184 0.9883582 ] 1 [0.8699379]\n",
      "✅ [0.00953551 0.99046445] 1 [0.6699379]\n",
      "✅ [0.00871464 0.9912854 ] 1 [0.4699379]\n",
      "✅ [0.01310259 0.9868974 ] 1 [0.2699379]\n",
      "✅ [0.03571119 0.96428883] 1 [0.0699379]\n",
      "❌ [0.1277517 0.8722483] 0 [0.]\n",
      "✅ [0.51808906 0.48191094] 0 [0.]\n",
      "✅ [0.869086   0.13091393] 0 [0.]\n",
      "✅ [0.95179963 0.04820037] 0 [0.]\n",
      "✅ [0.98034465 0.01965542] 0 [0.]\n",
      "✅ [0.99125695 0.00874302] 0 [0.]\n",
      "✅ [0.9954336  0.00456638] 0 [0.]\n",
      "✅ [0.9968803  0.00311969] 0 [0.]\n",
      "✅ [0.9967679  0.00323211] 0 [0.]\n",
      "✅ [0.9930561  0.00694395] 0 [0.]\n",
      "✅ [0.9716494  0.02835059] 0 [0.]\n",
      "✅ [0.8254026  0.17459744] 0 [0.]\n",
      "✅ [0.24547297 0.75452703] 1 [1.9399854]\n",
      "✅ [0.03512282 0.9648771 ] 1 [1.7399853]\n",
      "✅ [0.01844293 0.98155713] 1 [1.5399854]\n",
      "✅ [0.00915073 0.99084926] 1 [1.3399854]\n",
      "✅ [0.00673507 0.9932649 ] 1 [1.1399853]\n",
      "✅ [0.00542187 0.9945781 ] 1 [0.9399854]\n",
      "✅ [0.00581555 0.99418443] 1 [0.73998535]\n",
      "✅ [0.00937341 0.9906266 ] 1 [0.53998536]\n",
      "✅ [0.02904637 0.9709536 ] 1 [0.33998537]\n",
      "✅ [0.15149413 0.84850585] 1 [0.13998537]\n",
      "✅ [0.5766857  0.42331427] 0 [0.]\n",
      "✅ [0.8754954  0.12450467] 0 [0.]\n",
      "✅ [0.9654434  0.03455666] 0 [0.]\n",
      "✅ [0.9862701  0.01372985] 0 [0.]\n",
      "✅ [0.99325126 0.00674871] 0 [0.]\n",
      "✅ [0.99665266 0.00334736] 0 [0.]\n",
      "✅ [0.9959889  0.00401112] 0 [0.]\n",
      "✅ [0.9940235  0.00597645] 0 [0.]\n",
      "✅ [0.9788737  0.02112632] 0 [0.]\n",
      "✅ [0.90127295 0.09872706] 0 [0.]\n",
      "✅ [0.41919464 0.58080536] 1 [1.9399854]\n",
      "✅ [0.05409931 0.94590074] 1 [1.7399853]\n",
      "✅ [0.02242116 0.9775788 ] 1 [1.5399854]\n",
      "✅ [0.010883   0.98911697] 1 [1.3399854]\n",
      "✅ [0.00762878 0.9923712 ] 1 [1.1399853]\n",
      "✅ [0.00572633 0.9942736 ] 1 [0.9399854]\n",
      "✅ [0.00446033 0.9955396 ] 1 [0.73998535]\n",
      "✅ [0.00581511 0.9941849 ] 1 [0.53998536]\n",
      "✅ [0.01465791 0.985342  ] 1 [0.33998537]\n",
      "✅ [0.0644229 0.9355771] 1 [0.13998537]\n",
      "❌ [0.28360543 0.7163946 ] 0 [0.]\n",
      "✅ [0.62449586 0.37550414] 0 [0.]\n",
      "✅ [0.83304894 0.166951  ] 0 [0.]\n",
      "✅ [0.93647116 0.06352881] 0 [0.]\n",
      "✅ [0.9700827  0.02991726] 0 [0.]\n",
      "✅ [0.9861268  0.01387316] 0 [0.]\n",
      "✅ [0.9941831 0.0058169] 0 [0.]\n",
      "✅ [0.9947448  0.00525527] 0 [0.]\n",
      "✅ [0.99296814 0.00703185] 0 [0.]\n",
      "✅ [0.9784132  0.02158676] 0 [0.]\n",
      "✅ [0.9185258  0.08147418] 0 [0.]\n",
      "✅ [0.7116671 0.2883329] 0 [0.]\n",
      "✅ [0.22217473 0.77782524] 1 [1.8900087]\n",
      "✅ [0.06767303 0.932327  ] 1 [1.6900086]\n",
      "✅ [0.03552479 0.9644752 ] 1 [1.4900086]\n",
      "✅ [0.02854642 0.9714536 ] 1 [1.2900087]\n",
      "✅ [0.01975118 0.98024887] 1 [1.0900086]\n",
      "✅ [0.01312011 0.9868798 ] 1 [0.8900086]\n",
      "✅ [0.00811827 0.9918818 ] 1 [0.69000864]\n",
      "✅ [0.01033159 0.9896685 ] 1 [0.49000865]\n",
      "✅ [0.02637275 0.97362727] 1 [0.29000863]\n",
      "✅ [0.08069687 0.9193031 ] 1 [0.09000864]\n",
      "❌ [0.33227077 0.6677292 ] 0 [0.]\n",
      "✅ [0.69711804 0.302882  ] 0 [0.]\n",
      "✅ [0.9011008  0.09889915] 0 [0.]\n",
      "✅ [0.9681686  0.03183141] 0 [0.]\n",
      "✅ [0.9855387  0.01446123] 0 [0.]\n",
      "✅ [0.9932689 0.0067311] 0 [0.]\n",
      "✅ [0.996307   0.00369302] 0 [0.]\n",
      "✅ [0.9957331 0.0042669] 0 [0.]\n",
      "✅ [0.9916363 0.0083637] 0 [0.]\n",
      "✅ [0.97220796 0.02779206] 0 [0.]\n",
      "✅ [0.8652961  0.13470389] 0 [0.]\n",
      "✅ [0.51247984 0.48752016] 0 [0.]\n",
      "✅ [0.12344445 0.87655556] 1 [1.8699467]\n",
      "✅ [0.045201 0.954799] 1 [1.6699468]\n",
      "✅ [0.0205165 0.9794835] 1 [1.4699467]\n",
      "✅ [0.01330552 0.98669446] 1 [1.2699468]\n",
      "✅ [0.00878535 0.99121463] 1 [1.0699468]\n",
      "✅ [0.00757883 0.9924212 ] 1 [0.8699468]\n",
      "✅ [0.00695606 0.99304396] 1 [0.6699468]\n",
      "✅ [0.01051042 0.9894896 ] 1 [0.46994677]\n",
      "✅ [0.02386048 0.9761395 ] 1 [0.26994675]\n",
      "✅ [0.082257 0.917743] 1 [0.06994677]\n",
      "❌ [0.3368592  0.66314083] 0 [0.]\n",
      "✅ [0.66715276 0.33284727] 0 [0.]\n",
      "✅ [0.8598006  0.14019942] 0 [0.]\n",
      "✅ [0.96393216 0.03606783] 0 [0.]\n",
      "✅ [0.989783 0.010217] 0 [0.]\n",
      "✅ [0.9959092  0.00409083] 0 [0.]\n",
      "✅ [0.9983278  0.00167217] 0 [0.]\n",
      "✅ [0.9984268  0.00157325] 0 [0.]\n",
      "✅ [0.9986877 0.0013123] 0 [0.]\n",
      "✅ [0.9978713  0.00212868] 0 [0.]\n",
      "✅ [0.99683195 0.0031681 ] 0 [0.]\n",
      "✅ [0.996188   0.00381202] 0 [0.]\n",
      "✅ [0.9956648  0.00433528] 0 [0.]\n",
      "✅ [0.99433196 0.00566801] 0 [0.]\n",
      "✅ [0.9936546  0.00634543] 0 [0.]\n",
      "Test loss: 0.3873440949462102, Test accuracy: 93.58288770053476\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "testuuid = \"94ABBF79-C4A7-4715-A661-94BC30627CBD\"\n",
    "d_test, t_test = id.import_data(testuuid, 0.2, 100, 2)\n",
    "t_test_original = copy.deepcopy(t_test)\n",
    "t_test[t_test != 0] = 1\n",
    "t_test = np.sum(t_test, axis=1)\n",
    "t_test = np.squeeze(t_test, axis=-1)\n",
    "x_test = torch.tensor(d_test, dtype=torch.float32).to(device)\n",
    "test_dataset = SitUpDataset(x_test, t_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "        \n",
    "# Load the saved model\n",
    "model = SitUpModel(num_features, num_classes).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "total_test_loss = 0.0\n",
    "total_test_correct = 0\n",
    "batch_num = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        # Load the batch and conver to the correct datatypes\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.long().to(device)\n",
    "\n",
    "        # Calculate batch outputs and loss\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Calculate training stats\n",
    "        total_test_loss += loss.item() * x_batch.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_test_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        # Convert the outputs to a NumPy array\n",
    "        outputs = outputs.cpu().numpy()\n",
    "\n",
    "        for i, output in enumerate(outputs):\n",
    "            output_num_situps = np.argmax(output)\n",
    "            true_num_situps = y_batch[i]\n",
    "            if true_num_situps == output_num_situps:\n",
    "                verify_mark = \"✅\"\n",
    "            else:\n",
    "                verify_mark = \"❌\"\n",
    "            print(verify_mark, output, y_batch[i].item(), t_test_original[batch_num*64 + i].flatten())\n",
    "        batch_num += 1\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_dataset)\n",
    "test_accuracy = 100* total_test_correct / len(test_dataset)\n",
    "print(f\"Test loss: {avg_test_loss}, Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# # Run the model on the new dataset\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(test_data)\n",
    "#     total_test_loss = loss.item() * x_batch.size(0)\n",
    "#     _, predicted = torch.max(outputs, 1)\n",
    "#     total_eval_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52082/3400682453.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('checkpoint.pth')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     26\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader\u001b[49m:\n\u001b[1;32m     28\u001b[0m     x_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# # Load the checkpoint\n",
    "# checkpoint = torch.load('checkpoint.pth')\n",
    "\n",
    "# # Reinitialize the model and optimizer\n",
    "# model = SitUpModel(num_features=num_features, num_classes=num_classes).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# # Load the state dictionaries\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "# # Retrieve the last epoch\n",
    "# start_epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# # Define the number of additional epochs\n",
    "# additional_epochs = 300\n",
    "# total_epochs = start_epoch + additional_epochs\n",
    "\n",
    "# # Training loop  # Initial number of epochs\n",
    "# for epoch in range(start_epoch, total_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     correct = 0\n",
    "#     for x_batch, y_batch in dataloader:\n",
    "#         x_batch = x_batch.permute(0, 1, 2).to(device)\n",
    "#         y_batch = y_batch.long().to(device)\n",
    "#         # mask_batch = mask_batch.to(device)\n",
    "\n",
    "#         outputs = model(x_batch)\n",
    "#         # print(outputs)\n",
    "#         # print(y_batch)\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "\n",
    "#         total_loss += loss.item() * x_batch.size(0)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        \n",
    "\n",
    "#     scheduler.step()\n",
    "#     avg_loss = total_loss / len(dataset)\n",
    "#     accuracy = 100* correct / len(dataset)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%, learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "#         # print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# # for epoch in range(start_epoch, total_epochs):\n",
    "# #     model.train()\n",
    "# #     for x_batch, y_batch in dataloader:\n",
    "# #         x_batch = x_batch.permute(0, 1, 2).to(device)\n",
    "# #         y_batch = y_batch.to(device)\n",
    "# #         # mask_batch = mask_batch.to(device)\n",
    "\n",
    "# #         predictions = model(x_batch)\n",
    "# #         loss = mse_loss(predictions, y_batch)\n",
    "# #         optimizer.zero_grad()\n",
    "# #         loss.backward()\n",
    "# #         optimizer.step()\n",
    "\n",
    "# #     scheduler.step()\n",
    "\n",
    "# #     print(f\"Epoch {epoch + 1}/{total_epochs}, Loss: {loss.item():.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# # Save the model again after additional training\n",
    "# torch.save({\n",
    "#     'epoch': total_epochs,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'scheduler_state_dict': scheduler.state_dict(),\n",
    "#     'loss': loss.item(),\n",
    "# }, 'checkpoint.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
