{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find start time, skipping this dataset\n",
      "Could not find start time, skipping this dataset\n",
      "Could not find start time, skipping this dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n"
     ]
    }
   ],
   "source": [
    "import importData as import_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "uuids = [\"99AAC7DE-4106-43AB-AA80-F5816AB4DB85\", \"69508C29-2B15-4230-BE32-328E553EC63D\", \"2D2247C8-6B76-45BE-AC6B-F177A87ED4EF\"]\n",
    "id = import_data.importData()\n",
    "d_final, t_final = None, None\n",
    "for uuid in uuids:\n",
    "    d, t = id.import_data(uuid, 0.02, 100,2)\n",
    "    if d_final is None:\n",
    "        d_final, t_final = d, t\n",
    "    else:\n",
    "        # print(\"o\")\n",
    "        if len(t_final[0]) == len(t[0]):\n",
    "            d_final = np.concatenate((d_final, d), axis=0)\n",
    "            t_final = np.concatenate((t_final, t), axis=0)\n",
    "        else:\n",
    "            if len(t_final[0]) < len(t[0]):\n",
    "                difference = -len(t_final[0]) + len(t[0])\n",
    "                lengthened_t_final = []\n",
    "                for i, t_fin in enumerate(t_final):\n",
    "                    # print(np.shape(t_fin))\n",
    "                    zeros = np.zeros((difference,1))\n",
    "                    # print(np.shape(zeros))\n",
    "                    # print(t_fin)\n",
    "                    # print(zeros)\n",
    "                    lengthened_t_final.append(np.concatenate((t_fin, np.zeros((difference,1))), axis=0))\n",
    "                d_final = np.concatenate((d_final, d), axis=0)\n",
    "                t_final = np.concatenate((lengthened_t_final, t), axis=0)\n",
    "            elif len(t_final[0]) > len(t[0]):\n",
    "                difference = len(t_final[0]) - len(t[0])\n",
    "                lengthened_t= []\n",
    "                for i, t_fin in enumerate(t):\n",
    "                    # print(np.shape(t_fin))\n",
    "                    zeros = np.zeros((difference,1))\n",
    "                    # print(np.shape(zeros))\n",
    "                    # print(t_fin)\n",
    "                    # print(zeros)\n",
    "                    lengthened_t.append(np.concatenate((t_fin, np.zeros((difference,1))), axis=0))\n",
    "                d_final = np.concatenate((d_final, d), axis=0)\n",
    "                t_final = np.concatenate((t_final, lengthened_t), axis=0)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16431, 1, 1)\n",
      "[[0.02537209]\n",
      " [0.02745974]\n",
      " [0.01572108]\n",
      " [0.00457269]\n",
      " [0.00149256]]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(t_final))\n",
    "print((d_final[0][6])[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "X_train shape: (13144, 9, 200, 1)\n",
      "Y_train shape: (13144, 1, 1)\n",
      "X_val shape: (3287, 9, 200, 1)\n",
      "Y_val shape: (3287, 1, 1)\n",
      "Training model\n",
      "Epoch 10/100, Training Loss: 0.08313465863466263, Validation Loss: 0.08658119291067123\n",
      "Epoch 20/100, Training Loss: 0.08534877747297287, Validation Loss: 0.08323398232460022\n",
      "Epoch 30/100, Training Loss: 0.09372290968894958, Validation Loss: 0.0885239951312542\n",
      "Epoch 40/100, Training Loss: 0.07976388186216354, Validation Loss: 0.0745658241212368\n",
      "Epoch 50/100, Training Loss: 0.07867102324962616, Validation Loss: 0.07299650087952614\n",
      "Epoch 60/100, Training Loss: 0.07204126566648483, Validation Loss: 0.06742030009627342\n",
      "Epoch 70/100, Training Loss: 0.06890368461608887, Validation Loss: 0.06495996564626694\n",
      "Epoch 80/100, Training Loss: 0.057045940309762955, Validation Loss: 0.0573909766972065\n",
      "Epoch 90/100, Training Loss: 0.05835511162877083, Validation Loss: 0.05851021222770214\n",
      "Epoch 100/100, Training Loss: 0.05261973664164543, Validation Loss: 0.05438287556171417\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if   torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(d_final, t_final, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify that the split is done correctly\n",
    "print(f\"X_train shape: {np.shape(X_train)}\")\n",
    "print(f\"Y_train shape: {np.shape(Y_train)}\")\n",
    "print(f\"X_val shape: {np.shape(X_val)}\")\n",
    "print(f\"Y_val shape: {np.shape(Y_val)}\")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "Y_val = torch.tensor(Y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=3000, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=3000)\n",
    "\n",
    "# Define the model\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, max_seq_length):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv = nn.Conv2d(9, 128, kernel_size=(3, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1))  # Only pool along the height dimension\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(12672, 128)  # Adjusted dimensions after conv and pool\n",
    "        self.repeat = nn.Linear(128, 128 * max_seq_length)\n",
    "        self.lstm = nn.LSTM(128, 64, batch_first=True)\n",
    "        self.fc_out = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc(x))\n",
    "        x = self.repeat(x)\n",
    "        x = x.view(-1, max_seq_length, 128)  # Reshape to (batch_size, seq_length, input_size)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "max_seq_length = Y_train.shape[1]\n",
    "model = CNNLSTMModel(max_seq_length).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "print(\"Training model\")\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, Y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    if (epoch+1) % 10  == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"torch_model_2.pth\")\n",
    "\n",
    "# Convert the model to TorchScript\n",
    "model.eval() \n",
    "# An example input tensor with the same dimensions as the input data\n",
    "example_input = torch.rand(1, 9, 200, 1).to(device)  \n",
    "traced_script_module = torch.jit.trace(model, example_input)\n",
    "\n",
    "# Save the TorchScript model\n",
    "traced_script_module.save(\"torch_trace_model_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/Development/AATraining/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/Users/thomas/Development/AATraining/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions on new dataset:\n",
      "[[[0.00675528]]\n",
      "\n",
      " [[0.00703042]]\n",
      "\n",
      " [[0.00774223]]\n",
      "\n",
      " [[0.01036542]]\n",
      "\n",
      " [[0.01069561]]]\n",
      "test hypersuccess\n"
     ]
    }
   ],
   "source": [
    "testuuid = \"98006881-B2F0-4B6B-94CF-A21ABE69F21B\"\n",
    "\n",
    "\n",
    "d_test, t_test = id.import_data(testuuid, 0.02, 100, 2)\n",
    "indices = list(range(len(d_test)))\n",
    "sampled_indices = random.sample(indices, 1000)\n",
    "sampled_indices = sorted(sampled_indices)\n",
    "d_test, t_test = d_test[sampled_indices], t_test[sampled_indices]\n",
    "\n",
    "new_data = torch.tensor(d_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Load the saved model\n",
    "model = CNNLSTMModel(max_seq_length).to(device)\n",
    "model.load_state_dict(torch.load(\"torch_model_2.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Run the model on the new dataset\n",
    "with torch.no_grad():\n",
    "    outputs = model(new_data)\n",
    "\n",
    "# Convert the outputs to a NumPy array\n",
    "outputs = outputs.cpu().numpy()\n",
    "\n",
    "# Print the first few predictions\n",
    "print(\"Model predictions on new dataset:\")\n",
    "print(outputs[:5])\n",
    "print(\"test hypersuccess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00675528]] [[0.]]\n",
      "[[0.00703042]] [[0.]]\n",
      "[[0.00774223]] [[0.]]\n",
      "[[0.01036542]] [[0.]]\n",
      "[[0.01069561]] [[0.]]\n",
      "[[0.01018587]] [[0.]]\n",
      "[[0.00804317]] [[0.]]\n",
      "[[0.00707874]] [[0.]]\n",
      "[[0.00568781]] [[0.]]\n",
      "[[0.00563673]] [[0.]]\n",
      "[[0.00566428]] [[0.]]\n",
      "[[0.00922664]] [[0.]]\n",
      "[[0.00997469]] [[0.]]\n",
      "[[0.00546965]] [[0.]]\n",
      "[[0.00655639]] [[0.]]\n",
      "[[0.00617026]] [[0.]]\n",
      "[[0.00527724]] [[0.]]\n",
      "[[0.00554919]] [[0.]]\n",
      "[[0.00548293]] [[0.]]\n",
      "[[0.00535844]] [[0.]]\n",
      "[[0.00434169]] [[0.]]\n",
      "[[0.00403629]] [[0.]]\n",
      "[[0.0030123]] [[0.]]\n",
      "[[0.00582982]] [[0.]]\n",
      "[[0.00541848]] [[0.]]\n",
      "[[0.00480393]] [[0.]]\n",
      "[[0.00564097]] [[0.]]\n",
      "[[0.00768361]] [[0.]]\n",
      "[[0.00738056]] [[0.]]\n",
      "[[0.00570971]] [[0.]]\n",
      "[[0.00142682]] [[0.]]\n",
      "[[-0.00431833]] [[0.]]\n",
      "[[-0.00385856]] [[0.]]\n",
      "[[-0.00233629]] [[0.]]\n",
      "[[0.00174347]] [[0.]]\n",
      "[[0.0010265]] [[0.]]\n",
      "[[-0.00077491]] [[0.]]\n",
      "[[-0.00266827]] [[0.]]\n",
      "[[-0.00979718]] [[0.]]\n",
      "[[0.00872009]] [[0.]]\n",
      "[[-0.0031431]] [[0.]]\n",
      "[[-0.00253222]] [[0.]]\n",
      "[[-0.00307516]] [[0.]]\n",
      "[[-0.00163758]] [[0.]]\n",
      "[[-0.00154263]] [[0.]]\n",
      "[[-0.00308405]] [[0.]]\n",
      "[[-0.00346496]] [[0.]]\n",
      "[[-0.00425091]] [[0.]]\n",
      "[[-0.0061129]] [[0.]]\n",
      "[[-0.00788322]] [[0.]]\n",
      "[[-0.01065253]] [[0.]]\n",
      "[[-0.01032563]] [[0.]]\n",
      "[[-0.01027252]] [[0.]]\n",
      "[[-0.01048409]] [[0.]]\n",
      "[[-0.01701589]] [[0.]]\n",
      "[[-0.01719888]] [[0.]]\n",
      "[[-0.01711692]] [[0.]]\n",
      "[[-0.0032139]] [[0.]]\n",
      "[[0.00396973]] [[0.]]\n",
      "[[0.00127264]] [[0.]]\n",
      "[[0.00097232]] [[0.]]\n",
      "[[-0.00012404]] [[0.]]\n",
      "[[-0.00058854]] [[0.]]\n",
      "[[-0.00121374]] [[0.]]\n",
      "[[-0.00256184]] [[0.]]\n",
      "[[-0.00270947]] [[0.]]\n",
      "[[-0.00274196]] [[0.]]\n",
      "[[-0.00209334]] [[0.]]\n",
      "[[-0.00199265]] [[0.]]\n",
      "[[-0.00284476]] [[0.]]\n",
      "[[-0.00407176]] [[0.]]\n",
      "[[-0.00401799]] [[0.]]\n",
      "[[-0.00436199]] [[0.]]\n",
      "[[0.00293583]] [[0.]]\n",
      "[[0.01429022]] [[0.]]\n",
      "[[-0.02412654]] [[0.]]\n",
      "[[-0.06352779]] [[0.]]\n",
      "[[0.31412292]] [[0.]]\n",
      "[[0.22151676]] [[0.]]\n",
      "[[0.11483418]] [[0.]]\n",
      "[[0.12448262]] [[0.]]\n",
      "[[0.10043358]] [[0.]]\n",
      "[[0.21421558]] [[0.]]\n",
      "[[0.02506158]] [[0.]]\n",
      "[[-0.00449025]] [[0.]]\n",
      "[[-0.00504652]] [[0.]]\n",
      "[[-0.00284196]] [[0.]]\n",
      "[[-0.00157914]] [[0.]]\n",
      "[[-0.00194055]] [[0.]]\n",
      "[[-0.00199666]] [[0.]]\n",
      "[[-0.001978]] [[0.]]\n",
      "[[-0.00183963]] [[0.]]\n",
      "[[-0.00198986]] [[0.]]\n",
      "[[-0.00181054]] [[0.]]\n",
      "[[-0.00172351]] [[0.]]\n",
      "[[-0.0019013]] [[0.]]\n",
      "[[-0.00186888]] [[0.]]\n",
      "[[-0.00162899]] [[0.]]\n",
      "[[-0.00164169]] [[0.]]\n",
      "[[-0.00195175]] [[0.]]\n",
      "[[-0.00199255]] [[0.]]\n",
      "[[-0.00201061]] [[0.]]\n",
      "[[-0.00149809]] [[0.]]\n",
      "[[-0.00168853]] [[0.]]\n",
      "[[-0.00169875]] [[0.]]\n",
      "[[-0.00165375]] [[0.]]\n",
      "[[-0.00169335]] [[0.]]\n",
      "[[-0.00170699]] [[0.]]\n",
      "[[-0.00108786]] [[0.]]\n",
      "[[-0.0010762]] [[0.]]\n",
      "[[-0.00093405]] [[0.]]\n",
      "[[0.00018425]] [[0.]]\n",
      "[[0.00053502]] [[0.]]\n",
      "[[0.0001478]] [[0.]]\n",
      "[[0.00103839]] [[0.]]\n",
      "[[-0.0001617]] [[0.]]\n",
      "[[0.01446246]] [[0.]]\n",
      "[[0.02152107]] [[0.]]\n",
      "[[0.0167067]] [[0.]]\n",
      "[[0.01357698]] [[0.]]\n",
      "[[0.0118093]] [[0.]]\n",
      "[[0.0100527]] [[0.]]\n",
      "[[0.0064099]] [[0.]]\n",
      "[[0.00530542]] [[0.]]\n",
      "[[0.01224418]] [[0.]]\n",
      "[[0.0126423]] [[0.]]\n",
      "[[0.0132744]] [[0.]]\n",
      "[[0.01294876]] [[0.]]\n",
      "[[0.01438762]] [[0.]]\n",
      "[[0.01632491]] [[0.]]\n",
      "[[0.00867195]] [[0.]]\n",
      "[[0.00927288]] [[0.]]\n",
      "[[0.00860724]] [[0.]]\n",
      "[[0.00250679]] [[0.]]\n",
      "[[-0.00820895]] [[0.]]\n",
      "[[-0.005114]] [[0.]]\n",
      "[[-0.00037689]] [[0.]]\n",
      "[[-0.00061487]] [[0.]]\n",
      "[[0.00146906]] [[0.]]\n",
      "[[0.00653835]] [[0.]]\n",
      "[[-0.00820279]] [[0.]]\n",
      "[[-0.00491483]] [[0.]]\n",
      "[[-0.00475837]] [[0.]]\n",
      "[[-0.0054462]] [[0.]]\n",
      "[[-0.00715049]] [[0.]]\n",
      "[[-0.00370024]] [[0.]]\n",
      "[[0.00181835]] [[0.]]\n",
      "[[0.0103358]] [[0.]]\n",
      "[[0.00975084]] [[0.]]\n",
      "[[0.00625481]] [[0.]]\n",
      "[[0.00813639]] [[0.]]\n",
      "[[0.00811388]] [[0.]]\n",
      "[[0.00865673]] [[0.]]\n",
      "[[0.01124311]] [[0.]]\n",
      "[[0.01069773]] [[0.]]\n",
      "[[0.00706332]] [[0.]]\n",
      "[[0.00079359]] [[0.]]\n",
      "[[-0.00175063]] [[0.]]\n",
      "[[-0.12553117]] [[0.]]\n",
      "[[0.01486982]] [[0.]]\n",
      "[[0.02187214]] [[0.]]\n",
      "[[0.06197697]] [[0.]]\n",
      "[[0.16951218]] [[0.]]\n",
      "[[1.1709427]] [[1.880006]]\n",
      "[[1.3450661]] [[1.800006]]\n",
      "[[1.5434123]] [[1.760006]]\n",
      "[[1.6540546]] [[1.740006]]\n",
      "[[1.713384]] [[1.700006]]\n",
      "[[1.4203295]] [[1.4000059]]\n",
      "[[1.257054]] [[1.1600059]]\n",
      "[[1.1024858]] [[1.0400059]]\n",
      "[[0.8166473]] [[0.920006]]\n",
      "[[0.76006526]] [[0.76000595]]\n",
      "[[0.56559145]] [[0.640006]]\n",
      "[[0.52801836]] [[0.600006]]\n",
      "[[0.48787168]] [[0.580006]]\n",
      "[[0.30224222]] [[0.42000598]]\n",
      "[[0.12383466]] [[0.26000598]]\n",
      "[[0.09789019]] [[0.20000598]]\n",
      "[[0.00796378]] [[0.]]\n",
      "[[0.01606376]] [[0.]]\n",
      "[[-0.00390431]] [[0.]]\n",
      "[[0.00041075]] [[0.]]\n",
      "[[-0.00146259]] [[0.]]\n",
      "[[-0.00160114]] [[0.]]\n",
      "[[0.093407]] [[1.9300175]]\n",
      "[[-0.00713522]] [[1.9100175]]\n",
      "[[0.28876022]] [[1.8300174]]\n",
      "[[1.4691454]] [[1.6700175]]\n",
      "[[1.5039629]] [[1.4500175]]\n",
      "[[1.5066398]] [[1.4100175]]\n",
      "[[1.5077085]] [[1.3900175]]\n",
      "[[1.4002974]] [[1.2900175]]\n",
      "[[1.2088346]] [[1.0500175]]\n",
      "[[1.1684905]] [[0.9900175]]\n",
      "[[1.1138065]] [[0.9100175]]\n",
      "[[1.0985082]] [[0.8900175]]\n",
      "[[0.67488694]] [[0.4900175]]\n",
      "[[0.57993317]] [[0.4500175]]\n",
      "[[0.55783385]] [[0.4300175]]\n",
      "[[0.5104816]] [[0.3500175]]\n",
      "[[0.13680911]] [[0.1900175]]\n",
      "[[0.07972359]] [[0.1100175]]\n",
      "[[0.02629917]] [[0.0100175]]\n",
      "[[0.03366194]] [[0.]]\n",
      "[[0.04464108]] [[0.]]\n",
      "[[0.00232958]] [[0.]]\n",
      "[[0.00033206]] [[0.]]\n",
      "[[-0.00316153]] [[0.]]\n",
      "[[0.00094571]] [[0.]]\n",
      "[[0.00580995]] [[0.]]\n",
      "[[0.01767632]] [[1.9899588]]\n",
      "[[1.9807509]] [[1.9099588]]\n",
      "[[1.6419942]] [[1.7099588]]\n",
      "[[1.6521219]] [[1.6699588]]\n",
      "[[1.5554429]] [[1.5899588]]\n",
      "[[1.4961255]] [[1.4499588]]\n",
      "[[1.4798256]] [[1.4299588]]\n",
      "[[1.4845133]] [[1.3899589]]\n",
      "[[1.4232599]] [[1.2899588]]\n",
      "[[1.2727557]] [[1.1499588]]\n",
      "[[0.8606114]] [[0.7499588]]\n",
      "[[0.8638396]] [[0.66995883]]\n",
      "[[0.8716202]] [[0.6299588]]\n",
      "[[0.34930858]] [[0.2899588]]\n",
      "[[0.27494848]] [[0.22995882]]\n",
      "[[0.18545529]] [[0.1499588]]\n",
      "[[0.02518079]] [[0.]]\n",
      "[[0.07636716]] [[0.]]\n",
      "[[0.00212125]] [[0.]]\n",
      "[[0.00119261]] [[0.]]\n",
      "[[0.00620291]] [[0.]]\n",
      "[[0.00142852]] [[0.]]\n",
      "[[0.00056568]] [[0.]]\n",
      "[[0.00052776]] [[0.]]\n",
      "[[0.00451318]] [[0.]]\n",
      "[[0.15658501]] [[1.9599558]]\n",
      "[[1.7477486]] [[1.7999557]]\n",
      "[[1.6448518]] [[1.4999558]]\n",
      "[[1.4739053]] [[1.3799558]]\n",
      "[[1.2803003]] [[1.1399558]]\n",
      "[[1.2062091]] [[1.0799558]]\n",
      "[[1.0996498]] [[0.9199558]]\n",
      "[[1.0316314]] [[0.8399558]]\n",
      "[[0.966915]] [[0.81995577]]\n",
      "[[0.9398822]] [[0.75995576]]\n",
      "[[0.7797723]] [[0.51995575]]\n",
      "[[0.5134289]] [[0.19995578]]\n",
      "[[0.17564946]] [[0.01995578]]\n",
      "[[0.12460747]] [[0.]]\n",
      "[[0.04140606]] [[0.]]\n",
      "[[0.04795576]] [[0.]]\n",
      "[[0.06716312]] [[0.]]\n",
      "[[0.0575429]] [[0.]]\n",
      "[[0.01540937]] [[0.]]\n",
      "[[-0.00019076]] [[0.]]\n",
      "[[-0.00040904]] [[1.9891089]]\n",
      "[[0.00602735]] [[1.9291089]]\n",
      "[[0.00462724]] [[1.7891089]]\n",
      "[[1.5759362]] [[1.4091089]]\n",
      "[[1.4175195]] [[1.1491089]]\n",
      "[[1.3601571]] [[0.96910894]]\n",
      "[[1.2919953]] [[0.90910894]]\n",
      "[[1.1894957]] [[0.78910893]]\n",
      "[[1.1571487]] [[0.7491089]]\n",
      "[[1.0424169]] [[0.58910894]]\n",
      "[[0.9334835]] [[0.52910894]]\n",
      "[[0.9233379]] [[0.48910892]]\n",
      "[[0.89385074]] [[0.4691089]]\n",
      "[[0.72419566]] [[0.36910892]]\n",
      "[[0.7115332]] [[0.3091089]]\n",
      "[[0.6753291]] [[0.24910891]]\n",
      "[[0.09719516]] [[0.]]\n",
      "[[0.0548]] [[0.]]\n",
      "[[0.03801069]] [[0.]]\n",
      "[[0.02550719]] [[0.]]\n",
      "[[0.00336302]] [[0.]]\n",
      "[[0.00652143]] [[0.]]\n",
      "[[0.00566958]] [[0.]]\n",
      "[[0.00575703]] [[0.]]\n",
      "[[0.01112797]] [[0.]]\n",
      "[[0.01092242]] [[0.]]\n",
      "[[0.02754743]] [[0.]]\n",
      "[[0.03397875]] [[0.]]\n",
      "[[1.0299714]] [[1.7699152]]\n",
      "[[1.497677]] [[1.6899152]]\n",
      "[[1.7265973]] [[1.6699152]]\n",
      "[[1.7142025]] [[1.5899152]]\n",
      "[[1.6774458]] [[1.5499152]]\n",
      "[[1.5879719]] [[1.4899153]]\n",
      "[[1.5143224]] [[1.4299152]]\n",
      "[[1.5119263]] [[1.4099152]]\n",
      "[[1.503242]] [[1.3499151]]\n",
      "[[1.1609777]] [[0.9899152]]\n",
      "[[1.018413]] [[0.8099152]]\n",
      "[[0.9694258]] [[0.7699152]]\n",
      "[[0.9325034]] [[0.7499152]]\n",
      "[[0.8158692]] [[0.6099152]]\n",
      "[[0.7285446]] [[0.4899152]]\n",
      "[[0.6983897]] [[0.4499152]]\n",
      "[[0.54468715]] [[0.3499152]]\n",
      "[[0.07284058]] [[0.]]\n",
      "[[0.02696507]] [[0.]]\n",
      "[[0.01980265]] [[0.]]\n",
      "[[0.02205273]] [[0.]]\n",
      "[[0.02579275]] [[0.]]\n",
      "[[0.01803635]] [[0.]]\n",
      "[[0.01064558]] [[0.]]\n",
      "[[0.00720854]] [[0.]]\n",
      "[[0.002034]] [[0.]]\n",
      "[[0.00322342]] [[0.]]\n",
      "[[0.00251748]] [[0.]]\n",
      "[[0.01231097]] [[0.]]\n",
      "[[0.33487883]] [[0.]]\n",
      "[[1.8566328]] [[0.]]\n",
      "[[1.6396855]] [[1.9499161]]\n",
      "[[1.4720532]] [[1.9299161]]\n",
      "[[1.399308]] [[1.6299161]]\n",
      "[[1.08813]] [[1.149916]]\n",
      "[[1.0213618]] [[1.029916]]\n",
      "[[0.8532746]] [[0.8099161]]\n",
      "[[0.70544577]] [[0.6499161]]\n",
      "[[0.684862]] [[0.6099161]]\n",
      "[[0.62877023]] [[0.5499161]]\n",
      "[[-0.01078739]] [[0.]]\n",
      "[[0.04046164]] [[0.]]\n",
      "[[0.01967254]] [[0.]]\n",
      "[[0.03442612]] [[0.]]\n",
      "[[-0.00094339]] [[0.]]\n",
      "[[0.00063987]] [[0.]]\n",
      "[[0.00711358]] [[0.]]\n",
      "[[0.00869288]] [[0.]]\n",
      "[[0.0053461]] [[0.]]\n",
      "[[1.2315532]] [[0.]]\n",
      "[[1.0333781]] [[0.]]\n",
      "[[1.9833704]] [[1.8299857]]\n",
      "[[1.8761102]] [[1.7099857]]\n",
      "[[1.595149]] [[1.5699857]]\n",
      "[[1.5335046]] [[1.5299857]]\n",
      "[[1.4630084]] [[1.4699857]]\n",
      "[[1.4500939]] [[1.4499857]]\n",
      "[[1.4480646]] [[1.4299858]]\n",
      "[[1.4508767]] [[1.4099858]]\n",
      "[[1.2059019]] [[1.0899857]]\n",
      "[[1.0836904]] [[0.94998574]]\n",
      "[[0.9481586]] [[0.82998574]]\n",
      "[[0.83234763]] [[0.5499857]]\n",
      "[[0.67731017]] [[0.48998573]]\n",
      "[[0.5929818]] [[0.3899857]]\n",
      "[[0.39305395]] [[0.24998572]]\n",
      "[[0.08607084]] [[0.]]\n",
      "[[0.05150106]] [[0.]]\n",
      "[[0.02577826]] [[0.]]\n",
      "[[0.01037014]] [[0.]]\n",
      "[[0.0280527]] [[0.]]\n",
      "[[0.01730667]] [[0.]]\n",
      "[[0.01448599]] [[0.]]\n",
      "[[0.00020719]] [[0.]]\n",
      "[[0.00607438]] [[0.]]\n",
      "[[-0.00109452]] [[0.]]\n",
      "[[-0.00212746]] [[0.]]\n",
      "[[-0.00527325]] [[1.9896284]]\n",
      "[[-0.02781264]] [[1.9496285]]\n",
      "[[2.0124433]] [[1.8096285]]\n",
      "[[1.905938]] [[1.7096285]]\n",
      "[[1.3690495]] [[1.1296285]]\n",
      "[[1.008176]] [[0.76962847]]\n",
      "[[0.9532033]] [[0.6696285]]\n",
      "[[0.88203776]] [[0.58962846]]\n",
      "[[0.8938152]] [[0.5096285]]\n",
      "[[0.50430375]] [[0.2096285]]\n",
      "[[0.47496262]] [[0.18962848]]\n",
      "[[0.16955069]] [[0.]]\n",
      "[[0.13930371]] [[0.]]\n",
      "[[0.0543078]] [[0.]]\n",
      "[[0.04648448]] [[0.]]\n",
      "[[0.05173788]] [[0.]]\n",
      "[[0.02821753]] [[0.]]\n",
      "[[0.01883466]] [[0.]]\n",
      "[[0.00464084]] [[0.]]\n",
      "[[0.00376496]] [[0.]]\n",
      "[[0.0135702]] [[0.]]\n",
      "[[0.00953294]] [[0.]]\n",
      "[[0.0089091]] [[0.]]\n",
      "[[0.00814574]] [[0.]]\n",
      "[[0.00715293]] [[0.]]\n",
      "[[1.1990325]] [[1.4994783]]\n",
      "[[1.2821068]] [[1.4594784]]\n",
      "[[1.367492]] [[1.4194783]]\n",
      "[[1.4596204]] [[1.3594784]]\n",
      "[[1.4618144]] [[1.3194784]]\n",
      "[[1.5400938]] [[1.2794783]]\n",
      "[[1.5738813]] [[1.1194783]]\n",
      "[[1.5543987]] [[1.0994784]]\n",
      "[[1.4806875]] [[1.0394783]]\n",
      "[[1.278778]] [[0.91947836]]\n",
      "[[1.2681252]] [[0.8994784]]\n",
      "[[1.1713463]] [[0.7794784]]\n",
      "[[0.9909688]] [[0.6394783]]\n",
      "[[0.6971983]] [[0.43947834]]\n",
      "[[0.64695597]] [[0.39947835]]\n",
      "[[0.47166428]] [[0.35947835]]\n",
      "[[0.43942633]] [[0.27947834]]\n",
      "[[0.13799116]] [[0.01947835]]\n",
      "[[0.0790999]] [[0.]]\n",
      "[[0.02281899]] [[0.]]\n",
      "[[0.00505772]] [[0.]]\n",
      "[[0.00415689]] [[0.]]\n",
      "[[-0.00626919]] [[0.]]\n",
      "[[0.00141834]] [[0.]]\n",
      "[[-0.00226184]] [[0.]]\n",
      "[[-0.00309041]] [[0.]]\n",
      "[[0.00899857]] [[0.]]\n",
      "[[0.06827014]] [[0.]]\n",
      "[[1.2311256]] [[0.]]\n",
      "[[0.3241318]] [[1.7699949]]\n",
      "[[1.1471688]] [[1.6699948]]\n",
      "[[1.4709827]] [[1.6099949]]\n",
      "[[1.447612]] [[1.5099949]]\n",
      "[[1.4371674]] [[1.4899949]]\n",
      "[[1.3706614]] [[1.3499949]]\n",
      "[[1.3177105]] [[1.2899948]]\n",
      "[[1.2950721]] [[1.2099949]]\n",
      "[[1.0070682]] [[0.9699949]]\n",
      "[[0.92173284]] [[0.88999486]]\n",
      "[[0.77902895]] [[0.7899949]]\n",
      "[[0.76712805]] [[0.6699949]]\n",
      "[[0.70138294]] [[0.6499949]]\n",
      "[[0.3928291]] [[0.42999488]]\n",
      "[[0.18173337]] [[0.3499949]]\n",
      "[[0.15115324]] [[0.2899949]]\n",
      "[[0.10323719]] [[0.2299949]]\n",
      "[[0.05532596]] [[0.16999489]]\n",
      "[[0.01823702]] [[0.00999489]]\n",
      "[[0.00906007]] [[0.]]\n",
      "[[-0.0016368]] [[0.]]\n",
      "[[-0.00402162]] [[0.]]\n",
      "[[-0.00507426]] [[0.]]\n",
      "[[-0.02530097]] [[1.979946]]\n",
      "[[0.51312935]] [[1.839946]]\n",
      "[[0.78655064]] [[1.699946]]\n",
      "[[1.0103849]] [[1.659946]]\n",
      "[[0.42474663]] [[1.539946]]\n",
      "[[1.146346]] [[1.319946]]\n",
      "[[1.3092109]] [[1.219946]]\n",
      "[[1.309307]] [[1.199946]]\n",
      "[[1.2691537]] [[1.099946]]\n",
      "[[1.0082128]] [[0.919946]]\n",
      "[[0.8852644]] [[0.83994603]]\n",
      "[[0.82667506]] [[0.799946]]\n",
      "[[0.3497958]] [[0.299946]]\n",
      "[[0.20820531]] [[0.179946]]\n",
      "[[0.09372097]] [[0.09994601]]\n",
      "[[0.08092222]] [[0.03994601]]\n",
      "[[0.00452086]] [[0.]]\n",
      "[[0.00779273]] [[0.]]\n",
      "[[0.00824638]] [[0.]]\n",
      "[[-0.00016975]] [[0.]]\n",
      "[[-0.00064281]] [[0.]]\n",
      "[[0.00185]] [[0.]]\n",
      "[[0.00122249]] [[0.]]\n",
      "[[0.01203657]] [[0.]]\n",
      "[[0.8308735]] [[0.]]\n",
      "[[0.02114575]] [[0.]]\n",
      "[[0.1404179]] [[0.]]\n",
      "[[1.3835264]] [[1.9699957]]\n",
      "[[2.0262926]] [[1.9299958]]\n",
      "[[1.185171]] [[1.8699957]]\n",
      "[[1.5821987]] [[1.8499957]]\n",
      "[[1.4349116]] [[1.8299958]]\n",
      "[[1.4249316]] [[1.7899958]]\n",
      "[[0.9679674]] [[1.7499957]]\n",
      "[[1.4153234]] [[1.4699957]]\n",
      "[[1.3588969]] [[1.3899957]]\n",
      "[[1.1685854]] [[1.0699958]]\n",
      "[[0.9872235]] [[0.9299958]]\n",
      "[[0.8896766]] [[0.84999573]]\n",
      "[[0.90015584]] [[0.82999575]]\n",
      "[[0.8178674]] [[0.70999575]]\n",
      "[[0.12229948]] [[0.14999574]]\n",
      "[[0.02797847]] [[0.]]\n",
      "[[0.01525903]] [[0.]]\n",
      "[[0.00552533]] [[0.]]\n",
      "[[0.00445448]] [[0.]]\n",
      "[[0.01608307]] [[0.]]\n",
      "[[0.0038045]] [[0.]]\n",
      "[[0.00385495]] [[0.]]\n",
      "[[0.39753994]] [[0.]]\n",
      "[[0.63147205]] [[1.940002]]\n",
      "[[0.56913704]] [[1.860002]]\n",
      "[[0.30787796]] [[1.800002]]\n",
      "[[0.31463864]] [[1.700002]]\n",
      "[[0.6793982]] [[1.640002]]\n",
      "[[1.1517509]] [[1.520002]]\n",
      "[[1.1954836]] [[1.400002]]\n",
      "[[1.2646514]] [[1.360002]]\n",
      "[[1.2311083]] [[1.120002]]\n",
      "[[0.96461594]] [[0.920002]]\n",
      "[[0.804987]] [[0.820002]]\n",
      "[[0.78123635]] [[0.700002]]\n",
      "[[0.6134225]] [[0.600002]]\n",
      "[[0.54778063]] [[0.520002]]\n",
      "[[0.45471177]] [[0.44000202]]\n",
      "[[0.08938973]] [[0.20000201]]\n",
      "[[0.06801536]] [[0.12000201]]\n",
      "[[0.04499286]] [[0.]]\n",
      "[[0.02452912]] [[0.]]\n",
      "[[0.02136035]] [[0.]]\n",
      "[[0.01905439]] [[0.]]\n",
      "[[0.00982851]] [[0.]]\n",
      "[[0.01155405]] [[0.]]\n",
      "[[0.01697704]] [[0.]]\n",
      "[[0.00449304]] [[0.]]\n",
      "[[0.00072455]] [[0.]]\n",
      "[[0.00397068]] [[0.]]\n",
      "[[0.00652778]] [[0.]]\n",
      "[[1.2072185]] [[0.]]\n",
      "[[1.1686021]] [[1.8296443]]\n",
      "[[1.5297081]] [[1.6296443]]\n",
      "[[1.4313835]] [[1.4696443]]\n",
      "[[1.4161711]] [[1.4096442]]\n",
      "[[1.3945452]] [[1.3896443]]\n",
      "[[1.345921]] [[1.3496443]]\n",
      "[[1.3160938]] [[1.2496443]]\n",
      "[[1.3259009]] [[1.2096443]]\n",
      "[[1.0651015]] [[1.0096443]]\n",
      "[[0.77751195]] [[0.7496443]]\n",
      "[[0.66353357]] [[0.6696443]]\n",
      "[[0.5831559]] [[0.48964426]]\n",
      "[[0.1443786]] [[0.14964427]]\n",
      "[[0.09612953]] [[0.10964427]]\n",
      "[[0.05444039]] [[0.]]\n",
      "[[0.04586571]] [[0.]]\n",
      "[[0.03506198]] [[0.]]\n",
      "[[0.03853797]] [[0.]]\n",
      "[[0.02480625]] [[0.]]\n",
      "[[0.02280135]] [[0.]]\n",
      "[[0.02238873]] [[0.]]\n",
      "[[0.02160085]] [[0.]]\n",
      "[[0.01857638]] [[0.]]\n",
      "[[0.00145406]] [[0.]]\n",
      "[[-0.00151172]] [[0.]]\n",
      "[[-0.00534862]] [[0.]]\n",
      "[[-0.00470499]] [[0.]]\n",
      "[[0.00483353]] [[0.]]\n",
      "[[-0.00595256]] [[0.]]\n",
      "[[-0.09108847]] [[0.]]\n",
      "[[-0.04136971]] [[1.9999436]]\n",
      "[[0.17212766]] [[1.8799436]]\n",
      "[[1.4047875]] [[1.7599436]]\n",
      "[[1.4649569]] [[1.6399436]]\n",
      "[[1.4751542]] [[1.5999435]]\n",
      "[[1.4984521]] [[1.5199436]]\n",
      "[[1.4123665]] [[1.4799435]]\n",
      "[[1.1023259]] [[1.4399436]]\n",
      "[[0.59050417]] [[1.3799436]]\n",
      "[[0.32376736]] [[1.3399435]]\n",
      "[[0.74974245]] [[1.0399436]]\n",
      "[[0.38358006]] [[0.75994354]]\n",
      "[[0.40336475]] [[0.67994356]]\n",
      "[[0.01564084]] [[0.43994355]]\n",
      "[[0.01187561]] [[0.27994356]]\n",
      "[[0.01125274]] [[0.21994357]]\n",
      "[[0.02286173]] [[0.]]\n",
      "[[0.00572877]] [[0.]]\n",
      "[[-0.00343218]] [[0.]]\n",
      "[[-0.00260545]] [[0.]]\n",
      "[[0.00965399]] [[0.]]\n",
      "[[-0.12398726]] [[0.]]\n",
      "[[1.7437087]] [[0.]]\n",
      "[[1.6556498]] [[1.9699922]]\n",
      "[[1.2070068]] [[1.6899922]]\n",
      "[[1.4801127]] [[1.6099921]]\n",
      "[[1.1684141]] [[1.2499921]]\n",
      "[[1.069337]] [[1.1499921]]\n",
      "[[0.9916698]] [[1.1099921]]\n",
      "[[0.78891736]] [[0.84999216]]\n",
      "[[0.75746024]] [[0.8299921]]\n",
      "[[0.43638244]] [[0.5899921]]\n",
      "[[0.23671868]] [[0.46999213]]\n",
      "[[0.07631176]] [[0.32999215]]\n",
      "[[0.06011941]] [[0.24999213]]\n",
      "[[0.04670995]] [[0.14999214]]\n",
      "[[0.04305737]] [[0.10999213]]\n",
      "[[0.04695956]] [[0.]]\n",
      "[[0.03867535]] [[0.]]\n",
      "[[0.03818164]] [[0.]]\n",
      "[[0.02581535]] [[0.]]\n",
      "[[0.00122716]] [[0.]]\n",
      "[[-0.00040072]] [[0.]]\n",
      "[[0.0014616]] [[0.]]\n",
      "[[0.00639416]] [[0.]]\n",
      "[[0.007566]] [[0.]]\n",
      "[[-0.01624363]] [[0.]]\n",
      "[[-0.15482926]] [[0.]]\n",
      "[[-0.11137105]] [[0.]]\n",
      "[[0.07114946]] [[0.]]\n",
      "[[0.65150434]] [[0.]]\n",
      "[[0.64289993]] [[0.]]\n",
      "[[0.4511973]] [[1.9695268]]\n",
      "[[1.254846]] [[1.6495267]]\n",
      "[[1.0637597]] [[1.5895268]]\n",
      "[[1.286426]] [[1.4095267]]\n",
      "[[1.2184674]] [[1.2095268]]\n",
      "[[1.0300814]] [[1.0695268]]\n",
      "[[0.9826392]] [[1.0495267]]\n",
      "[[0.7215326]] [[0.74952674]]\n",
      "[[0.48201695]] [[0.54952675]]\n",
      "[[0.4416075]] [[0.5095267]]\n",
      "[[0.19093814]] [[0.32952675]]\n",
      "[[0.15456915]] [[0.30952674]]\n",
      "[[0.05903331]] [[0.12952673]]\n",
      "[[0.01952228]] [[0.]]\n",
      "[[0.00913523]] [[0.]]\n",
      "[[0.00758851]] [[0.]]\n",
      "[[0.00862339]] [[0.]]\n",
      "[[0.00615836]] [[0.]]\n",
      "[[0.00538193]] [[0.]]\n",
      "[[-0.00241051]] [[0.]]\n",
      "[[-0.0017372]] [[0.]]\n",
      "[[0.03227973]] [[0.]]\n",
      "[[-0.02369603]] [[0.]]\n",
      "[[-0.00701822]] [[0.]]\n",
      "[[-0.10979678]] [[0.]]\n",
      "[[-0.12296564]] [[1.9791126]]\n",
      "[[0.7727676]] [[1.8991127]]\n",
      "[[1.3778931]] [[1.7791127]]\n",
      "[[1.2463218]] [[1.7591127]]\n",
      "[[1.2749473]] [[1.7391126]]\n",
      "[[0.57100046]] [[1.6791127]]\n",
      "[[1.10043]] [[1.4591126]]\n",
      "[[1.3335907]] [[1.2791127]]\n",
      "[[1.0699159]] [[0.95911264]]\n",
      "[[0.70632243]] [[0.57911265]]\n",
      "[[0.68562]] [[0.55911267]]\n",
      "[[0.44008407]] [[0.43911266]]\n",
      "[[0.4363589]] [[0.39911267]]\n",
      "[[0.3263162]] [[0.2591127]]\n",
      "[[0.07440227]] [[0.03911267]]\n",
      "[[0.03418916]] [[0.]]\n",
      "[[0.01195603]] [[0.]]\n",
      "[[0.01105054]] [[0.]]\n",
      "[[0.01052103]] [[0.]]\n",
      "[[0.00191534]] [[0.]]\n",
      "[[-0.00123009]] [[0.]]\n",
      "[[-0.0013041]] [[0.]]\n",
      "[[0.00508361]] [[0.]]\n",
      "[[-0.00039033]] [[0.]]\n",
      "[[-0.03684282]] [[0.]]\n",
      "[[-0.27854714]] [[0.]]\n",
      "[[-0.2665083]] [[1.9298017]]\n",
      "[[0.74639046]] [[1.8298018]]\n",
      "[[1.201837]] [[1.5498017]]\n",
      "[[1.364755]] [[1.4898018]]\n",
      "[[1.4095899]] [[1.4098017]]\n",
      "[[1.4099633]] [[1.3698018]]\n",
      "[[1.237774]] [[1.1898017]]\n",
      "[[1.1437815]] [[1.1098018]]\n",
      "[[0.7338933]] [[0.64980173]]\n",
      "[[0.62249696]] [[0.5898017]]\n",
      "[[0.47966245]] [[0.50980175]]\n",
      "[[0.44611326]] [[0.46980175]]\n",
      "[[0.38624752]] [[0.38980174]]\n",
      "[[0.01801118]] [[0.10980175]]\n",
      "[[0.01579263]] [[0.08980175]]\n",
      "[[0.00978305]] [[0.04980175]]\n",
      "[[0.00683257]] [[0.]]\n",
      "[[0.01570173]] [[0.]]\n",
      "[[0.01648039]] [[0.]]\n",
      "[[0.00187708]] [[0.]]\n",
      "[[0.00409845]] [[0.]]\n",
      "[[0.00054885]] [[0.]]\n",
      "[[-0.04393532]] [[0.]]\n",
      "[[-0.05358798]] [[0.]]\n",
      "[[1.6796865]] [[0.]]\n",
      "[[2.0231962]] [[1.980004]]\n",
      "[[1.971656]] [[1.960004]]\n",
      "[[1.8586675]] [[1.820004]]\n",
      "[[1.65303]] [[1.7800039]]\n",
      "[[1.2612077]] [[1.720004]]\n",
      "[[1.3045754]] [[1.700004]]\n",
      "[[1.1818265]] [[1.5400039]]\n",
      "[[1.3760612]] [[1.4200039]]\n",
      "[[1.3714083]] [[1.4000039]]\n",
      "[[1.3277401]] [[1.3000039]]\n",
      "[[1.1556702]] [[1.0400039]]\n",
      "[[0.8761977]] [[0.80000395]]\n",
      "[[0.42689842]] [[0.48000392]]\n",
      "[[0.37081254]] [[0.42000392]]\n",
      "[[0.13293904]] [[0.28000394]]\n",
      "[[0.13398317]] [[0.26000392]]\n",
      "[[0.06141774]] [[0.16000393]]\n",
      "[[0.05914325]] [[0.12000393]]\n",
      "[[0.0117997]] [[0.]]\n",
      "[[0.01823103]] [[0.]]\n",
      "[[0.00345311]] [[0.]]\n",
      "[[0.00280316]] [[0.]]\n",
      "[[0.00212193]] [[0.]]\n",
      "[[0.00156975]] [[0.]]\n",
      "[[-0.00338373]] [[0.]]\n",
      "[[0.01820549]] [[0.]]\n",
      "[[0.01864804]] [[0.]]\n",
      "[[-0.09163777]] [[0.]]\n",
      "[[-0.03302104]] [[0.]]\n",
      "[[1.520137]] [[1.979994]]\n",
      "[[1.09594]] [[1.799994]]\n",
      "[[0.59104574]] [[1.739994]]\n",
      "[[1.4081286]] [[1.499994]]\n",
      "[[1.4016167]] [[1.479994]]\n",
      "[[1.3643931]] [[1.399994]]\n",
      "[[1.2528437]] [[1.239994]]\n",
      "[[1.1427511]] [[1.179994]]\n",
      "[[0.9515369]] [[1.079994]]\n",
      "[[0.903941]] [[1.019994]]\n",
      "[[0.6077765]] [[0.75999403]]\n",
      "[[0.54663265]] [[0.719994]]\n",
      "[[0.48646513]] [[0.599994]]\n",
      "[[0.29836744]] [[0.45999402]]\n",
      "[[0.16908267]] [[0.35999402]]\n",
      "[[0.06770165]] [[0.19999403]]\n",
      "[[0.03841241]] [[0.15999402]]\n",
      "[[0.03131395]] [[0.]]\n",
      "[[0.01971535]] [[0.]]\n",
      "[[0.01634685]] [[0.]]\n",
      "[[0.00605473]] [[0.]]\n",
      "[[0.01212041]] [[0.]]\n",
      "[[0.01244241]] [[0.]]\n",
      "[[-0.00039928]] [[0.]]\n",
      "[[0.00720179]] [[0.]]\n",
      "[[0.01388637]] [[0.]]\n",
      "[[1.9415323]] [[1.8799907]]\n",
      "[[1.7459303]] [[1.7799907]]\n",
      "[[1.0698583]] [[1.6799908]]\n",
      "[[0.86474264]] [[1.6199907]]\n",
      "[[1.0877723]] [[1.5799907]]\n",
      "[[1.49726]] [[1.3999907]]\n",
      "[[1.4845458]] [[1.3399907]]\n",
      "[[1.3370488]] [[1.1399907]]\n",
      "[[0.9009551]] [[0.8799907]]\n",
      "[[0.42361477]] [[0.3999907]]\n",
      "[[0.36251172]] [[0.33999074]]\n",
      "[[0.3271421]] [[0.2999907]]\n",
      "[[0.15837911]] [[0.15999073]]\n",
      "[[0.11108913]] [[0.11999072]]\n",
      "[[0.01133276]] [[0.]]\n",
      "[[0.01646107]] [[0.]]\n",
      "[[0.01134125]] [[0.]]\n",
      "[[0.01100088]] [[0.]]\n",
      "[[0.01146882]] [[0.]]\n",
      "[[0.01159695]] [[0.]]\n",
      "[[0.00982755]] [[0.]]\n",
      "[[0.01161856]] [[0.]]\n",
      "[[0.00571186]] [[0.]]\n",
      "[[0.00462604]] [[0.]]\n",
      "[[-0.00229528]] [[0.]]\n",
      "[[-0.00044684]] [[0.]]\n",
      "[[0.01230248]] [[0.]]\n",
      "[[1.6496277]] [[1.8292524]]\n",
      "[[1.3678054]] [[1.4092524]]\n",
      "[[1.338568]] [[1.3692523]]\n",
      "[[1.3514224]] [[1.3092524]]\n",
      "[[1.3206049]] [[1.1292523]]\n",
      "[[1.1452359]] [[0.9892524]]\n",
      "[[1.0004894]] [[0.88925236]]\n",
      "[[0.9158204]] [[0.84925234]]\n",
      "[[0.7305497]] [[0.5692524]]\n",
      "[[0.68193626]] [[0.52925235]]\n",
      "[[0.6156092]] [[0.30925238]]\n",
      "[[0.19409558]] [[0.14925237]]\n",
      "[[0.04581162]] [[0.]]\n",
      "[[0.02063743]] [[0.]]\n",
      "[[0.02367742]] [[0.]]\n",
      "[[0.00991545]] [[0.]]\n",
      "[[0.03527918]] [[0.]]\n",
      "[[0.04772282]] [[0.]]\n",
      "[[0.0366392]] [[0.]]\n",
      "[[0.01743549]] [[0.]]\n",
      "[[-0.00291454]] [[0.]]\n",
      "[[0.73103034]] [[0.]]\n",
      "[[1.4531296]] [[0.]]\n",
      "[[0.06575144]] [[0.]]\n",
      "[[1.5812413]] [[1.7100009]]\n",
      "[[0.69163746]] [[1.4700009]]\n",
      "[[0.87574387]] [[1.4300009]]\n",
      "[[1.3535146]] [[1.3700008]]\n",
      "[[1.4266316]] [[1.2300009]]\n",
      "[[1.3401488]] [[1.1100008]]\n",
      "[[1.0710083]] [[0.8900009]]\n",
      "[[0.83705765]] [[0.7700009]]\n",
      "[[0.7709596]] [[0.6100009]]\n",
      "[[0.67554575]] [[0.5700009]]\n",
      "[[0.64768374]] [[0.53000087]]\n",
      "[[0.43582556]] [[0.33000088]]\n",
      "[[0.24536166]] [[0.23000088]]\n",
      "[[0.15174815]] [[0.05000089]]\n",
      "[[0.01638319]] [[0.]]\n",
      "[[0.02205479]] [[0.]]\n",
      "[[0.02292287]] [[0.]]\n",
      "[[0.01006901]] [[0.]]\n",
      "[[0.01965655]] [[0.]]\n",
      "[[4.4628978e-05]] [[0.]]\n",
      "[[-0.01919343]] [[0.]]\n",
      "[[0.45397106]] [[0.]]\n",
      "[[1.2958624]] [[0.]]\n",
      "[[1.4040722]] [[1.9099966]]\n",
      "[[1.3364745]] [[1.8499966]]\n",
      "[[1.3528622]] [[1.4699966]]\n",
      "[[1.2969972]] [[1.3499966]]\n",
      "[[1.4174967]] [[1.2499967]]\n",
      "[[1.4231988]] [[1.2099966]]\n",
      "[[1.312263]] [[1.0899966]]\n",
      "[[1.05537]] [[0.9099966]]\n",
      "[[1.0310625]] [[0.88999665]]\n",
      "[[0.97051847]] [[0.8499966]]\n",
      "[[0.95361924]] [[0.82999665]]\n",
      "[[0.48178163]] [[0.44999662]]\n",
      "[[0.38411462]] [[0.30999663]]\n",
      "[[0.36163142]] [[0.24999662]]\n",
      "[[0.15404543]] [[0.14999662]]\n",
      "[[0.04498035]] [[0.]]\n",
      "[[0.04506696]] [[0.]]\n",
      "[[0.03410783]] [[0.]]\n",
      "[[0.02223757]] [[0.]]\n",
      "[[0.0090292]] [[0.]]\n",
      "[[0.0056033]] [[0.]]\n",
      "[[0.00295104]] [[0.]]\n",
      "[[0.00828513]] [[0.]]\n",
      "[[0.04663035]] [[0.]]\n",
      "[[0.0077128]] [[0.]]\n",
      "[[0.00886437]] [[0.]]\n",
      "[[-0.00439125]] [[0.]]\n",
      "[[0.00468954]] [[0.]]\n",
      "[[-0.0221742]] [[0.]]\n",
      "[[-0.11621533]] [[0.]]\n",
      "[[-0.04252393]] [[1.9790705]]\n",
      "[[1.7228147]] [[1.7990706]]\n",
      "[[1.5885133]] [[1.7590705]]\n",
      "[[0.5066237]] [[1.6390705]]\n",
      "[[0.5045575]] [[1.6190705]]\n",
      "[[0.90502316]] [[1.4990705]]\n",
      "[[1.3481923]] [[1.2390705]]\n",
      "[[1.2598222]] [[1.0990705]]\n",
      "[[0.75977945]] [[0.7590706]]\n",
      "[[0.48956951]] [[0.49907055]]\n",
      "[[0.46577403]] [[0.45907056]]\n",
      "[[0.1571765]] [[0.11907056]]\n",
      "[[0.09965112]] [[0.07907056]]\n",
      "[[0.04233371]] [[0.]]\n",
      "[[0.01889966]] [[0.]]\n",
      "[[0.01574364]] [[0.]]\n",
      "[[0.01459684]] [[0.]]\n",
      "[[0.01412045]] [[0.]]\n",
      "[[0.00245584]] [[0.]]\n",
      "[[0.08043278]] [[1.959945]]\n",
      "[[0.5777595]] [[1.9199449]]\n",
      "[[1.3178633]] [[1.839945]]\n",
      "[[1.4151826]] [[1.6199449]]\n",
      "[[1.465539]] [[1.3799449]]\n",
      "[[1.3855889]] [[1.2999449]]\n",
      "[[1.2792222]] [[1.219945]]\n",
      "[[1.2441388]] [[1.179945]]\n",
      "[[0.7541559]] [[0.6399449]]\n",
      "[[0.3713347]] [[0.41994494]]\n",
      "[[0.30587038]] [[0.35994494]]\n",
      "[[0.23123094]] [[0.29994494]]\n",
      "[[0.11179588]] [[0.15994494]]\n",
      "[[0.06477516]] [[0.09994493]]\n",
      "[[0.02722853]] [[0.]]\n",
      "[[0.04081276]] [[0.]]\n",
      "[[0.5630429]] [[0.]]\n",
      "[[-0.07205878]] [[0.]]\n",
      "[[0.10834581]] [[1.9195415]]\n",
      "[[1.7253214]] [[1.6595415]]\n",
      "[[1.5422143]] [[1.6195414]]\n",
      "[[1.4257407]] [[1.5395415]]\n",
      "[[1.4410444]] [[1.2795415]]\n",
      "[[1.0319709]] [[0.87954146]]\n",
      "[[0.78324294]] [[0.7395415]]\n",
      "[[0.7782188]] [[0.71954143]]\n",
      "[[0.50301576]] [[0.31954145]]\n",
      "[[0.45201543]] [[0.29954144]]\n",
      "[[0.37666807]] [[0.19954145]]\n",
      "[[0.365188]] [[0.17954145]]\n",
      "[[0.10552879]] [[0.]]\n",
      "[[0.02412622]] [[0.]]\n",
      "[[-0.02436209]] [[0.]]\n",
      "[[-0.0060214]] [[0.]]\n",
      "[[0.00190416]] [[0.]]\n",
      "[[-0.00130662]] [[0.]]\n",
      "[[0.00281988]] [[0.]]\n",
      "[[0.00027926]] [[0.]]\n",
      "[[-0.00069451]] [[0.]]\n",
      "[[-0.00074459]] [[0.]]\n",
      "[[-0.00033462]] [[0.]]\n",
      "[[0.00130428]] [[0.]]\n",
      "[[0.00117721]] [[0.]]\n",
      "[[-0.00114215]] [[0.]]\n",
      "[[-0.00305406]] [[0.]]\n",
      "[[-0.00255252]] [[0.]]\n",
      "[[-0.00205667]] [[0.]]\n",
      "[[0.00754584]] [[0.]]\n",
      "[[0.00973577]] [[0.]]\n",
      "[[0.01008906]] [[0.]]\n",
      "[[0.00062263]] [[0.]]\n",
      "[[-0.00517061]] [[0.]]\n",
      "[[-0.00486727]] [[0.]]\n",
      "[[-0.0055512]] [[0.]]\n",
      "[[-0.00367711]] [[0.]]\n",
      "[[5.5857003e-05]] [[0.]]\n",
      "[[0.00916557]] [[0.]]\n",
      "[[0.00542898]] [[0.]]\n",
      "[[0.00783027]] [[0.]]\n",
      "[[0.0079261]] [[0.]]\n",
      "[[0.00653507]] [[0.]]\n",
      "[[-0.00213943]] [[0.]]\n",
      "[[0.00318433]] [[0.]]\n",
      "[[0.00228643]] [[0.]]\n",
      "[[0.00558413]] [[0.]]\n",
      "[[6.482005e-07]] [[0.]]\n",
      "[[0.01252254]] [[0.]]\n",
      "[[0.010845]] [[0.]]\n",
      "[[0.01019514]] [[0.]]\n",
      "[[0.01111428]] [[0.]]\n",
      "[[0.01022638]] [[0.]]\n",
      "[[0.00670951]] [[0.]]\n",
      "[[0.00351862]] [[0.]]\n",
      "[[0.00282612]] [[0.]]\n",
      "[[0.00373641]] [[0.]]\n",
      "[[0.00402161]] [[0.]]\n",
      "[[0.00426468]] [[0.]]\n",
      "[[0.00436524]] [[0.]]\n",
      "[[0.00442681]] [[0.]]\n",
      "[[0.00413011]] [[0.]]\n",
      "[[0.00394237]] [[0.]]\n",
      "[[0.00409083]] [[0.]]\n",
      "[[0.00407799]] [[0.]]\n",
      "[[0.00410049]] [[0.]]\n",
      "[[0.00393392]] [[0.]]\n",
      "[[0.00387555]] [[0.]]\n",
      "[[0.00398119]] [[0.]]\n",
      "[[0.00412129]] [[0.]]\n",
      "[[0.00399587]] [[0.]]\n",
      "[[0.00372476]] [[0.]]\n",
      "[[0.00248057]] [[0.]]\n",
      "[[0.00218215]] [[0.]]\n",
      "[[0.00246759]] [[0.]]\n",
      "[[0.00255582]] [[0.]]\n",
      "[[0.00256167]] [[0.]]\n",
      "[[0.00224559]] [[0.]]\n",
      "[[0.00216934]] [[0.]]\n",
      "[[0.00210635]] [[0.]]\n",
      "[[0.00186563]] [[0.]]\n",
      "[[0.00156333]] [[0.]]\n",
      "[[0.00145569]] [[0.]]\n",
      "[[0.00107801]] [[0.]]\n",
      "[[0.00141009]] [[0.]]\n",
      "[[0.00186438]] [[0.]]\n",
      "[[0.00190204]] [[0.]]\n",
      "[[0.00162742]] [[0.]]\n",
      "[[0.0011087]] [[0.]]\n",
      "[[0.00108337]] [[0.]]\n",
      "[[0.00109863]] [[0.]]\n",
      "[[0.00098721]] [[0.]]\n",
      "[[0.00076089]] [[0.]]\n",
      "[[0.00077241]] [[0.]]\n",
      "[[0.00095967]] [[0.]]\n",
      "[[0.00192498]] [[0.]]\n",
      "[[0.00198622]] [[0.]]\n",
      "[[0.00227705]] [[0.]]\n",
      "[[0.00266759]] [[0.]]\n",
      "[[0.00247946]] [[0.]]\n",
      "[[0.00221651]] [[0.]]\n",
      "[[0.00167896]] [[0.]]\n",
      "[[0.00168078]] [[0.]]\n",
      "[[0.00484408]] [[0.]]\n",
      "[[0.00830307]] [[0.]]\n",
      "[[0.00892856]] [[0.]]\n",
      "[[0.00251731]] [[0.]]\n",
      "[[-0.00105385]] [[0.]]\n",
      "[[-0.00114823]] [[0.]]\n",
      "[[-0.0018476]] [[0.]]\n",
      "[[-0.01019081]] [[0.]]\n",
      "[[-0.04424343]] [[0.]]\n",
      "[[0.00780521]] [[0.]]\n",
      "[[0.0067388]] [[0.]]\n",
      "[[0.0070636]] [[0.]]\n",
      "[[-0.00332934]] [[0.]]\n",
      "[[-0.00395408]] [[0.]]\n",
      "[[-0.00455127]] [[0.]]\n",
      "[[-0.00459754]] [[0.]]\n",
      "[[-2.9809773e-05]] [[0.]]\n",
      "[[0.00185951]] [[0.]]\n",
      "[[0.00205672]] [[0.]]\n",
      "[[0.00314957]] [[0.]]\n",
      "[[0.00697742]] [[0.]]\n",
      "[[-0.01764213]] [[0.]]\n",
      "[[-0.02934913]] [[0.]]\n",
      "[[-0.06179374]] [[0.]]\n",
      "[[-0.06711898]] [[0.]]\n",
      "[[-0.07617604]] [[0.]]\n"
     ]
    }
   ],
   "source": [
    "for i, output in enumerate(outputs):\n",
    "    print(output, t_test[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
