{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]\n",
      "\n",
      " [[0.]]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n"
     ]
    }
   ],
   "source": [
    "import importData as import_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "np.set_printoptions(edgeitems=10)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "uuids = [\"69508C29-2B15-4230-BE32-328E553EC63D\", \"2D2247C8-6B76-45BE-AC6B-F177A87ED4EF\", \"98006881-B2F0-4B6B-94CF-A21ABE69F21B\", \"94ABBF79-C4A7-4715-A661-94BC30627CBD\", \"DC6DB199-0AA7-441E-B896-C70A34CF7B6C\", \"2F6A5D84-2523-4EE2-BF3D-7693665B2258\", \"9026576A-3EA6-4A42-8A7A-0FD1290CF5C9\", \"BAB130C6-7D85-489A-B52E-CB0E149E585D\"]\n",
    "id = import_data.importData()\n",
    "d_final, t_final = None, None\n",
    "for uuid in uuids:\n",
    "    d, t = id.import_data(uuid, 0.2, 100, 2)\n",
    "    if d_final is None:\n",
    "        d_final, t_final = d, t\n",
    "    else:\n",
    "        # print(\"o\")\n",
    "        if len(t_final[0]) == len(t[0]):\n",
    "            d_final = np.concatenate((d_final, d), axis=0)\n",
    "            t_final = np.concatenate((t_final, t), axis=0)\n",
    "        else:\n",
    "            if len(t_final[0]) < len(t[0]):\n",
    "                difference = -len(t_final[0]) + len(t[0])\n",
    "                lengthened_t_final = []\n",
    "                for i, t_fin in enumerate(t_final):\n",
    "                    # print(np.shape(t_fin))\n",
    "                    zeros = np.zeros((difference,1))\n",
    "                    # print(np.shape(zeros))\n",
    "                    # print(t_fin)\n",
    "                    # print(zeros)\n",
    "                    lengthened_t_final.append(np.concatenate((t_fin, np.zeros((difference,1))), axis=0))\n",
    "                d_final = np.concatenate((d_final, d), axis=0)\n",
    "                t_final = np.concatenate((lengthened_t_final, t), axis=0)\n",
    "            elif len(t_final[0]) > len(t[0]):\n",
    "                difference = len(t_final[0]) - len(t[0])\n",
    "                lengthened_t= []\n",
    "                for i, t_fin in enumerate(t):\n",
    "                    # print(np.shape(t_fin))\n",
    "                    zeros = np.zeros((difference,1))\n",
    "                    # print(np.shape(zeros))\n",
    "                    # print(t_fin)\n",
    "                    # print(zeros)\n",
    "                    lengthened_t.append(np.concatenate((t_fin, np.zeros((difference,1))), axis=0))\n",
    "                d_final = np.concatenate((d_final, d), axis=0)\n",
    "                t_final = np.concatenate((t_final, lengthened_t), axis=0)\n",
    "print(t_final)             \n",
    "t_final[t_final != 0] = 1\n",
    "\n",
    "t_final = np.sum(t_final, axis=1)\n",
    "print(t_final)\n",
    "t_final = np.squeeze(t_final, axis=-1)\n",
    "print(t_final)\n",
    "\n",
    "\n",
    "\n",
    "testuuid = \"99AAC7DE-4106-43AB-AA80-F5816AB4DB85\"\n",
    "d_test, t_test = id.import_data(testuuid, 0.2, 100, 2)\n",
    "t_test_original = copy.deepcopy(t_test)\n",
    "t_test[t_test != 0] = 1\n",
    "t_test = np.sum(t_test, axis=1)\n",
    "t_test = np.squeeze(t_test, axis=-1)\n",
    "# indices = list(range(len(d_test)))\n",
    "# sampled_indices = random.sample(indices, 1000)\n",
    "# sampled_indices = sorted(sampled_indices)\n",
    "# d_test, t_test = d_test[sampled_indices], t_test[sampled_indices]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Elements:  [0. 1.]\n",
      "Counts:  [3593 1090]\n"
     ]
    }
   ],
   "source": [
    "unique_t, counts = np.unique(t_final, return_counts=True)\n",
    "print(\"Unique Elements: \", unique_t)\n",
    "print(\"Counts: \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4636, 9, 200, 1)\n",
      "number of classes: 2\n",
      "number of features: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52082/4060731017.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
      "/tmp/ipykernel_52082/4060731017.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.targets = torch.tensor(targets, dtype=torch.float32)\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.4187, Train Accuracy: 92.64%, Val Loss: 0.4531, Val Accuracy: 88.10%, learning rate: 0.001000\n",
      "Epoch 2/50, Train Loss: 0.3799, Train Accuracy: 94.37%, Val Loss: 0.4266, Val Accuracy: 90.29%, learning rate: 0.001000\n",
      "Epoch 3/50, Train Loss: 0.3718, Train Accuracy: 94.93%, Val Loss: 0.4561, Val Accuracy: 85.53%, learning rate: 0.001000\n",
      "Epoch 4/50, Train Loss: 0.3725, Train Accuracy: 94.93%, Val Loss: 0.4625, Val Accuracy: 84.80%, learning rate: 0.001000\n",
      "Epoch 5/50, Train Loss: 0.3745, Train Accuracy: 94.46%, Val Loss: 0.4358, Val Accuracy: 88.10%, learning rate: 0.000500\n",
      "Epoch 6/50, Train Loss: 0.3676, Train Accuracy: 95.28%, Val Loss: 0.4509, Val Accuracy: 87.36%, learning rate: 0.000500\n",
      "Epoch 7/50, Train Loss: 0.3664, Train Accuracy: 95.47%, Val Loss: 0.4174, Val Accuracy: 91.03%, learning rate: 0.000500\n",
      "Epoch 8/50, Train Loss: 0.3718, Train Accuracy: 94.59%, Val Loss: 0.4171, Val Accuracy: 91.39%, learning rate: 0.000500\n",
      "Epoch 9/50, Train Loss: 0.3690, Train Accuracy: 95.19%, Val Loss: 0.4506, Val Accuracy: 86.26%, learning rate: 0.000500\n",
      "Epoch 10/50, Train Loss: 0.3685, Train Accuracy: 95.32%, Val Loss: 0.4642, Val Accuracy: 85.53%, learning rate: 0.000250\n",
      "Epoch 11/50, Train Loss: 0.3666, Train Accuracy: 95.34%, Val Loss: 0.4265, Val Accuracy: 90.66%, learning rate: 0.000250\n",
      "Epoch 12/50, Train Loss: 0.3652, Train Accuracy: 95.73%, Val Loss: 0.4567, Val Accuracy: 86.63%, learning rate: 0.000250\n",
      "Epoch 13/50, Train Loss: 0.3655, Train Accuracy: 95.41%, Val Loss: 0.4537, Val Accuracy: 86.81%, learning rate: 0.000250\n",
      "Epoch 14/50, Train Loss: 0.3693, Train Accuracy: 95.06%, Val Loss: 0.4547, Val Accuracy: 86.26%, learning rate: 0.000250\n",
      "Epoch 15/50, Train Loss: 0.3681, Train Accuracy: 95.23%, Val Loss: 0.4696, Val Accuracy: 85.35%, learning rate: 0.000125\n",
      "Epoch 16/50, Train Loss: 0.3643, Train Accuracy: 95.97%, Val Loss: 0.4204, Val Accuracy: 91.21%, learning rate: 0.000125\n",
      "Epoch 17/50, Train Loss: 0.3633, Train Accuracy: 95.92%, Val Loss: 0.4164, Val Accuracy: 91.58%, learning rate: 0.000125\n",
      "Epoch 18/50, Train Loss: 0.3632, Train Accuracy: 95.88%, Val Loss: 0.4105, Val Accuracy: 92.49%, learning rate: 0.000125\n",
      "Epoch 19/50, Train Loss: 0.3638, Train Accuracy: 95.92%, Val Loss: 0.4137, Val Accuracy: 91.94%, learning rate: 0.000125\n",
      "Epoch 20/50, Train Loss: 0.3630, Train Accuracy: 95.97%, Val Loss: 0.4566, Val Accuracy: 86.63%, learning rate: 0.000063\n",
      "Epoch 21/50, Train Loss: 0.3623, Train Accuracy: 96.01%, Val Loss: 0.4138, Val Accuracy: 91.94%, learning rate: 0.000063\n",
      "Epoch 22/50, Train Loss: 0.3628, Train Accuracy: 96.07%, Val Loss: 0.4097, Val Accuracy: 91.94%, learning rate: 0.000063\n",
      "Epoch 23/50, Train Loss: 0.3599, Train Accuracy: 96.31%, Val Loss: 0.4197, Val Accuracy: 91.58%, learning rate: 0.000063\n",
      "Epoch 24/50, Train Loss: 0.3624, Train Accuracy: 95.90%, Val Loss: 0.4256, Val Accuracy: 91.03%, learning rate: 0.000063\n",
      "Epoch 25/50, Train Loss: 0.3619, Train Accuracy: 95.99%, Val Loss: 0.4172, Val Accuracy: 91.58%, learning rate: 0.000031\n",
      "Epoch 26/50, Train Loss: 0.3605, Train Accuracy: 96.23%, Val Loss: 0.4133, Val Accuracy: 91.94%, learning rate: 0.000031\n",
      "Epoch 27/50, Train Loss: 0.3600, Train Accuracy: 96.31%, Val Loss: 0.4122, Val Accuracy: 92.12%, learning rate: 0.000031\n",
      "Epoch 28/50, Train Loss: 0.3601, Train Accuracy: 96.42%, Val Loss: 0.4135, Val Accuracy: 91.39%, learning rate: 0.000031\n",
      "Epoch 29/50, Train Loss: 0.3594, Train Accuracy: 96.42%, Val Loss: 0.4172, Val Accuracy: 91.58%, learning rate: 0.000031\n",
      "Epoch 30/50, Train Loss: 0.3602, Train Accuracy: 96.40%, Val Loss: 0.4182, Val Accuracy: 91.76%, learning rate: 0.000016\n",
      "Epoch 31/50, Train Loss: 0.3592, Train Accuracy: 96.35%, Val Loss: 0.4111, Val Accuracy: 93.04%, learning rate: 0.000016\n",
      "Epoch 32/50, Train Loss: 0.3593, Train Accuracy: 96.38%, Val Loss: 0.4132, Val Accuracy: 91.76%, learning rate: 0.000016\n",
      "Epoch 33/50, Train Loss: 0.3585, Train Accuracy: 96.70%, Val Loss: 0.4158, Val Accuracy: 91.39%, learning rate: 0.000016\n",
      "Epoch 34/50, Train Loss: 0.3590, Train Accuracy: 96.48%, Val Loss: 0.4102, Val Accuracy: 92.12%, learning rate: 0.000016\n",
      "Epoch 35/50, Train Loss: 0.3580, Train Accuracy: 96.66%, Val Loss: 0.4179, Val Accuracy: 91.39%, learning rate: 0.000008\n",
      "Epoch 36/50, Train Loss: 0.3583, Train Accuracy: 96.72%, Val Loss: 0.4134, Val Accuracy: 91.94%, learning rate: 0.000008\n",
      "Epoch 37/50, Train Loss: 0.3588, Train Accuracy: 96.46%, Val Loss: 0.4172, Val Accuracy: 92.12%, learning rate: 0.000008\n",
      "Epoch 38/50, Train Loss: 0.3585, Train Accuracy: 96.59%, Val Loss: 0.4128, Val Accuracy: 92.12%, learning rate: 0.000008\n",
      "Epoch 39/50, Train Loss: 0.3591, Train Accuracy: 96.59%, Val Loss: 0.4190, Val Accuracy: 91.21%, learning rate: 0.000008\n",
      "Epoch 40/50, Train Loss: 0.3591, Train Accuracy: 96.46%, Val Loss: 0.4170, Val Accuracy: 91.39%, learning rate: 0.000004\n",
      "Epoch 41/50, Train Loss: 0.3598, Train Accuracy: 96.35%, Val Loss: 0.4158, Val Accuracy: 91.76%, learning rate: 0.000004\n",
      "Epoch 42/50, Train Loss: 0.3582, Train Accuracy: 96.85%, Val Loss: 0.4128, Val Accuracy: 91.94%, learning rate: 0.000004\n",
      "Epoch 43/50, Train Loss: 0.3585, Train Accuracy: 96.40%, Val Loss: 0.4160, Val Accuracy: 92.31%, learning rate: 0.000004\n",
      "Epoch 44/50, Train Loss: 0.3583, Train Accuracy: 96.53%, Val Loss: 0.4157, Val Accuracy: 92.12%, learning rate: 0.000004\n",
      "Epoch 45/50, Train Loss: 0.3582, Train Accuracy: 96.53%, Val Loss: 0.4163, Val Accuracy: 92.12%, learning rate: 0.000002\n",
      "Epoch 46/50, Train Loss: 0.3592, Train Accuracy: 96.61%, Val Loss: 0.4108, Val Accuracy: 91.76%, learning rate: 0.000002\n",
      "Epoch 47/50, Train Loss: 0.3587, Train Accuracy: 96.44%, Val Loss: 0.4146, Val Accuracy: 92.12%, learning rate: 0.000002\n",
      "Epoch 48/50, Train Loss: 0.3597, Train Accuracy: 96.42%, Val Loss: 0.4164, Val Accuracy: 92.31%, learning rate: 0.000002\n",
      "Epoch 49/50, Train Loss: 0.3579, Train Accuracy: 96.74%, Val Loss: 0.4138, Val Accuracy: 92.12%, learning rate: 0.000002\n",
      "Epoch 50/50, Train Loss: 0.3581, Train Accuracy: 96.55%, Val Loss: 0.4138, Val Accuracy: 92.31%, learning rate: 0.000001\n"
     ]
    }
   ],
   "source": [
    "class SitUpDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (numpy array): Input data of shape (num_examples, num_features, num_readings, 1)\n",
    "            targets (numpy array): Target data of shape (num_examples, num_outputs, 1)\n",
    "            max_outputs (int): Maximum number of outputs per example\n",
    "        \"\"\"\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        # self.max_outputs = max_outputs\n",
    "\n",
    "        # Replace missing target times with -1 and create a mask\n",
    "        # self.masks = (self.targets > 0).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Remove the last dimension of size 1\n",
    "        x = self.inputs[idx].squeeze(-1)  # Shape: (num_features, num_readings)\n",
    "        y = self.targets[idx]  # Shape: (num_outputs)\n",
    "        # mask = self.masks[idx].squeeze(-1)  # Shape: (num_outputs)\n",
    "        return x, y#, mask\n",
    "\n",
    "# class SitUpModel(nn.Module):\n",
    "#     def __init__(self, num_features, num_outputs):\n",
    "#         super(SitUpModel, self).__init__()\n",
    "#         self.num_outputs = num_outputs\n",
    "\n",
    "#         # Define the model layers\n",
    "#         self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=5, padding=2)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "#         self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "#         self.fc = nn.Linear(128, num_outputs)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x shape: (batch_size, num_features, num_readings)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.pool(x)  # x shape: (batch_size, 128, 1)\n",
    "#         x = x.view(x.size(0), -1)  # Flatten to (batch_size, 128)\n",
    "#         x = self.fc(x)  # Output shape: (batch_size, num_outputs)\n",
    "#         return x\n",
    "\n",
    "# class SitUpModel(nn.Module):\n",
    "#     def __init__(self, num_features, num_outputs):\n",
    "#         super(SitUpModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(num_features, 64, kernel_size=5, padding=2)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.lstm = nn.LSTM(input_size=64, hidden_size=128, batch_first=True)\n",
    "#         self.fc = nn.Linear(128, num_outputs)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # x shape: (batch_size, num_features, sequence_length)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = x.permute(0, 2, 1)  # Reshape to (batch_size, sequence_length, features)\n",
    "#         out, _ = self.lstm(x)\n",
    "#         out = out[:, -1, :]  # Get the last output\n",
    "#         out = self.fc(out)\n",
    "#         return out\n",
    "\n",
    "# class SitUpModel(nn.Module):\n",
    "#     def __init__(self, num_features):\n",
    "#         super(SitUpCountModel, self).__init__()\n",
    "        \n",
    "#         # Define the model layers\n",
    "#         self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=5, padding=2)\n",
    "#         self.bn1 = nn.BatchNorm1d(64)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "#         self.bn2 = nn.BatchNorm1d(128)\n",
    "#         self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "#         self.fc = nn.Linear(128, 1)  # Output is a single value\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # x shape: (batch_size, num_features, num_readings)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.pool(x)  # x shape: (batch_size, 128, 1)\n",
    "#         x = x.view(x.size(0), -1)  # Flatten to (batch_size, 128)\n",
    "#         x = self.fc(x)  # Output shape: (batch_size, 1)\n",
    "#         x = x.squeeze(-1)  # Shape: (batch_size,)\n",
    "#         return x\n",
    "\n",
    "class SitUpModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(SitUpModel, self).__init__()\n",
    "        \n",
    "        # Define the model layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=7, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        self.sm = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_features, num_readings)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)  # x shape: (batch_size, 128, 1)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 128)\n",
    "        x = self.fc(x)  # Output shape: (batch_size, num_classes)\n",
    "        x = self.sm(x)\n",
    "        return x\n",
    "    \n",
    "def mse_loss(predictions, targets):\n",
    "    # Compute MSE loss only over valid targets\n",
    "    diff = (predictions - targets)\n",
    "    loss = (diff ** 2).sum() / len(predictions)\n",
    "    return loss\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    x_test = torch.tensor(d_test, dtype=torch.float32).to(device)\n",
    "    test_dataset = SitUpDataset(x_test, t_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Split Data into validation and training sets\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(d_final, t_final, test_size=0.01)\n",
    "    print(np.shape(X_train))\n",
    "\n",
    "    # Gather information on the dataset\n",
    "    num_classes = int(np.max(Y_train, axis=0) + 1)\n",
    "    num_features = len(X_train[0])\n",
    "    print(f\"number of classes: {num_classes}\")\n",
    "    print(f\"number of features: {num_features}\")\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    Y_train = torch.tensor(Y_train, dtype=torch.float32).to(device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    Y_val = torch.tensor(Y_val, dtype=torch.float32).to(device)\n",
    " \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = SitUpDataset(X_train, Y_train)\n",
    "    val_dataset = SitUpDataset(X_val, Y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "    # Initialize the model, optimizer, and scheduler with new learning rate\n",
    "    model = SitUpModel(num_features=num_features, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 50  # Initial number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        total_train_correct = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # Load the batch and conver to the correct datatypes\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.long().to(device)\n",
    "\n",
    "            # Calculate batch outputs and loss\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Optimise for the next step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate training stats\n",
    "            total_train_loss += loss.item() * x_batch.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_eval_loss = 0.0\n",
    "        total_eval_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                # Load the batch and conver to the correct datatypes\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.long().to(device)\n",
    "\n",
    "                # Calculate batch outputs and loss\n",
    "                outputs = model(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # Calculate training stats\n",
    "                total_eval_loss += loss.item() * x_batch.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_eval_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_train_loss = total_train_loss / len(train_dataset)\n",
    "        train_accuracy = 100* total_train_correct / len(train_dataset)\n",
    "        avg_val_loss = total_eval_loss / len(test_dataset)\n",
    "        val_accuracy = 100* total_eval_correct / len(test_dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    # Save the trained model and optimizer state\n",
    "    torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss.item(),\n",
    "    }, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_raw_df[\"time\"] = trimmed_raw_df[\"time\"].apply(lambda x: x - timestamps_df[\"startTime\"][0])\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/importData.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  start_time_dataset[\"time\"] = start_time_dataset[\"time\"] - start_time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [0.97411996 0.02587998] 0 [0.]\n",
      "✅ [0.9775994  0.02240066] 0 [0.]\n",
      "✅ [0.985446   0.01455399] 0 [0.]\n",
      "✅ [0.98322636 0.01677359] 0 [0.]\n",
      "✅ [0.96694887 0.03305119] 0 [0.]\n",
      "❌ [0.90081394 0.09918606] 1 [1.949934]\n",
      "❌ [0.63422436 0.36577561] 1 [1.749934]\n",
      "✅ [0.18115172 0.8188483 ] 1 [1.549934]\n",
      "✅ [0.03359068 0.9664093 ] 1 [1.349934]\n",
      "✅ [0.01392325 0.9860767 ] 1 [1.149934]\n",
      "✅ [0.01893852 0.98106146] 1 [0.949934]\n",
      "✅ [0.01816821 0.9818318 ] 1 [0.749934]\n",
      "✅ [0.01431333 0.9856867 ] 1 [0.549934]\n",
      "✅ [0.02223166 0.97776836] 1 [0.349934]\n",
      "✅ [0.04611637 0.9538836 ] 1 [0.14993401]\n",
      "❌ [0.12522139 0.87477857] 0 [0.]\n",
      "❌ [0.38708642 0.61291355] 0 [0.]\n",
      "✅ [0.73565125 0.26434872] 0 [0.]\n",
      "✅ [0.91376865 0.08623134] 0 [0.]\n",
      "✅ [0.9678872  0.03211275] 0 [0.]\n",
      "✅ [0.98560864 0.01439138] 0 [0.]\n",
      "✅ [0.9931594  0.00684055] 0 [0.]\n",
      "✅ [0.99655056 0.00344948] 0 [0.]\n",
      "✅ [0.9978258  0.00217421] 0 [0.]\n",
      "✅ [0.9977836  0.00221637] 0 [0.]\n",
      "✅ [0.9980323  0.00196774] 0 [0.]\n",
      "✅ [0.99479795 0.00520205] 0 [0.]\n",
      "✅ [0.9867949  0.01320513] 0 [0.]\n",
      "✅ [0.9588184  0.04118167] 0 [0.]\n",
      "❌ [0.7722497  0.22775029] 1 [1.9699576]\n",
      "✅ [0.3491978  0.65080214] 1 [1.7699575]\n",
      "✅ [0.07141551 0.9285845 ] 1 [1.5699575]\n",
      "✅ [0.03266269 0.9673373 ] 1 [1.3699576]\n",
      "✅ [0.02090063 0.97909933] 1 [1.1699575]\n",
      "✅ [0.01567948 0.9843205 ] 1 [0.96995753]\n",
      "✅ [0.01870212 0.98129785] 1 [0.76995754]\n",
      "✅ [0.02229117 0.9777089 ] 1 [0.56995755]\n",
      "✅ [0.03022149 0.9697785 ] 1 [0.36995754]\n",
      "✅ [0.07937923 0.92062074] 1 [0.16995755]\n",
      "❌ [0.24753088 0.75246906] 0 [0.]\n",
      "✅ [0.5842071  0.41579282] 0 [0.]\n",
      "✅ [0.8409165  0.15908355] 0 [0.]\n",
      "✅ [0.93376327 0.06623678] 0 [0.]\n",
      "✅ [0.9665041  0.03349585] 0 [0.]\n",
      "✅ [0.9815714  0.01842861] 0 [0.]\n",
      "✅ [0.988513   0.01148699] 0 [0.]\n",
      "✅ [0.9883633  0.01163672] 0 [0.]\n",
      "✅ [0.9867845 0.0132155] 0 [0.]\n",
      "✅ [0.9733974  0.02660263] 0 [0.]\n",
      "✅ [0.9597808  0.04021918] 0 [0.]\n",
      "❌ [0.84617114 0.15382886] 1 [1.9600236]\n",
      "✅ [0.32810163 0.67189837] 1 [1.7600236]\n",
      "✅ [0.05229096 0.9477091 ] 1 [1.5600237]\n",
      "✅ [0.02569088 0.97430915] 1 [1.3600236]\n",
      "✅ [0.01641756 0.9835825 ] 1 [1.1600237]\n",
      "✅ [0.01163865 0.9883613 ] 1 [0.96002364]\n",
      "✅ [0.00947426 0.9905257 ] 1 [0.76002365]\n",
      "✅ [0.01284394 0.98715603] 1 [0.56002367]\n",
      "✅ [0.02463338 0.9753666 ] 1 [0.36002365]\n",
      "✅ [0.0729316 0.9270684] 1 [0.16002364]\n",
      "❌ [0.25953063 0.7404694 ] 0 [0.]\n",
      "✅ [0.7140104  0.28598958] 0 [0.]\n",
      "✅ [0.9284603  0.07153969] 0 [0.]\n",
      "✅ [0.9722748  0.02772527] 0 [0.]\n",
      "✅ [0.98387355 0.01612649] 0 [0.]\n",
      "✅ [0.990321   0.00967898] 0 [0.]\n",
      "✅ [0.99423766 0.00576229] 0 [0.]\n",
      "✅ [0.99604344 0.00395663] 0 [0.]\n",
      "✅ [0.9943288  0.00567117] 0 [0.]\n",
      "✅ [0.9910897  0.00891025] 0 [0.]\n",
      "✅ [0.9582312  0.04176878] 0 [0.]\n",
      "❌ [0.67038727 0.32961276] 1 [1.8699946]\n",
      "✅ [0.19888441 0.8011156 ] 1 [1.6699947]\n",
      "✅ [0.04922056 0.95077944] 1 [1.4699947]\n",
      "✅ [0.02824152 0.9717585 ] 1 [1.2699947]\n",
      "✅ [0.01883889 0.9811612 ] 1 [1.0699947]\n",
      "✅ [0.01511093 0.98488903] 1 [0.8699947]\n",
      "✅ [0.01656606 0.98343396] 1 [0.6699947]\n",
      "✅ [0.02818791 0.97181207] 1 [0.4699947]\n",
      "✅ [0.08636943 0.9136306 ] 1 [0.26999468]\n",
      "✅ [0.36527136 0.63472867] 1 [0.06999469]\n",
      "✅ [0.80503076 0.19496927] 0 [0.]\n",
      "✅ [0.94420487 0.05579513] 0 [0.]\n",
      "✅ [0.9721485  0.02785147] 0 [0.]\n",
      "✅ [0.9812417  0.01875827] 0 [0.]\n",
      "✅ [0.9876581  0.01234184] 0 [0.]\n",
      "✅ [0.99124545 0.00875454] 0 [0.]\n",
      "✅ [0.9903516  0.00964833] 0 [0.]\n",
      "✅ [0.98678976 0.01321029] 0 [0.]\n",
      "✅ [0.9705364  0.02946367] 0 [0.]\n",
      "✅ [0.839856   0.16014396] 0 [0.]\n",
      "✅ [0.3384806 0.6615194] 1 [1.9399483]\n",
      "✅ [0.09967324 0.9003267 ] 1 [1.7399484]\n",
      "✅ [0.05831483 0.9416852 ] 1 [1.5399483]\n",
      "✅ [0.0396171  0.96038294] 1 [1.3399484]\n",
      "✅ [0.02822254 0.9717775 ] 1 [1.1399484]\n",
      "✅ [0.02427674 0.97572327] 1 [0.9399484]\n",
      "✅ [0.02992882 0.9700712 ] 1 [0.7399484]\n",
      "✅ [0.04166643 0.95833355] 1 [0.53994834]\n",
      "✅ [0.10791207 0.89208794] 1 [0.33994836]\n",
      "✅ [0.4050633  0.59493667] 1 [0.13994837]\n",
      "✅ [0.80111164 0.1988884 ] 0 [0.]\n",
      "✅ [0.93560296 0.06439702] 0 [0.]\n",
      "✅ [0.9661679  0.03383211] 0 [0.]\n",
      "✅ [0.97518253 0.02481746] 0 [0.]\n",
      "✅ [0.9808412  0.01915884] 0 [0.]\n",
      "✅ [0.98161626 0.01838372] 0 [0.]\n",
      "✅ [0.9740902  0.02590978] 0 [0.]\n",
      "✅ [0.9186926  0.08130737] 0 [0.]\n",
      "✅ [0.7291877 0.2708122] 0 [0.]\n",
      "✅ [0.3119528 0.6880472] 1 [1.8700032]\n",
      "✅ [0.07781083 0.9221892 ] 1 [1.6700032]\n",
      "✅ [0.04220244 0.95779765] 1 [1.4700032]\n",
      "✅ [0.03474208 0.96525794] 1 [1.2700032]\n",
      "✅ [0.02834745 0.97165257] 1 [1.0700033]\n",
      "✅ [0.02769578 0.9723042 ] 1 [0.8700032]\n",
      "✅ [0.03207018 0.9679298 ] 1 [0.67000324]\n",
      "✅ [0.05473774 0.94526225] 1 [0.47000322]\n",
      "✅ [0.14189279 0.8581072 ] 1 [0.27000323]\n",
      "❌ [0.55763316 0.44236687] 1 [0.07000323]\n",
      "✅ [0.878816   0.12118395] 0 [0.]\n",
      "✅ [0.9592282  0.04077178] 0 [0.]\n",
      "✅ [0.9746773  0.02532271] 0 [0.]\n",
      "✅ [0.98106927 0.01893081] 0 [0.]\n",
      "✅ [0.9867698  0.01323017] 0 [0.]\n",
      "✅ [0.9917772  0.00822286] 0 [0.]\n",
      "✅ [0.99193984 0.00806017] 0 [0.]\n",
      "✅ [0.9841234  0.01587653] 0 [0.]\n",
      "✅ [0.9701385  0.02986153] 0 [0.]\n",
      "✅ [0.822591   0.17740896] 0 [0.]\n",
      "✅ [0.39957243 0.60042757] 1 [1.9599826]\n",
      "✅ [0.06090083 0.9390992 ] 1 [1.7599826]\n",
      "✅ [0.02449291 0.9755071 ] 1 [1.5599827]\n",
      "✅ [0.0163017  0.98369825] 1 [1.3599826]\n",
      "✅ [0.01350688 0.9864932 ] 1 [1.1599827]\n",
      "✅ [0.01084022 0.98915976] 1 [0.95998263]\n",
      "✅ [0.01121215 0.98878783] 1 [0.75998265]\n",
      "✅ [0.02140006 0.97859997] 1 [0.55998266]\n",
      "✅ [0.04554411 0.9544559 ] 1 [0.35998264]\n",
      "✅ [0.20639105 0.7936089 ] 1 [0.15998264]\n",
      "✅ [0.61534065 0.38465938] 0 [0.]\n",
      "✅ [0.8760447  0.12395531] 0 [0.]\n",
      "✅ [0.9598053  0.04019472] 0 [0.]\n",
      "✅ [0.98122096 0.01877909] 0 [0.]\n",
      "✅ [0.99177414 0.00822593] 0 [0.]\n",
      "✅ [0.9966955  0.00330445] 0 [0.]\n",
      "✅ [0.9983411  0.00165885] 0 [0.]\n",
      "✅ [0.9984118 0.0015882] 0 [0.]\n",
      "✅ [0.9980956  0.00190441] 0 [0.]\n",
      "✅ [0.9969824  0.00301759] 0 [0.]\n",
      "✅ [0.9915876  0.00841241] 0 [0.]\n",
      "✅ [0.9730382  0.02696176] 0 [0.]\n",
      "✅ [0.86771655 0.13228348] 0 [0.]\n",
      "❌ [0.3707029 0.6292971] 0 [0.]\n",
      "✅ [0.07040206 0.9295979 ] 1 [1.8899958]\n",
      "✅ [0.02416524 0.9758347 ] 1 [1.6899958]\n",
      "✅ [0.01499521 0.9850048 ] 1 [1.4899957]\n",
      "✅ [0.01276935 0.9872306 ] 1 [1.2899958]\n",
      "✅ [0.0103058 0.9896941] 1 [1.0899957]\n",
      "✅ [0.012104   0.98789597] 1 [0.88999575]\n",
      "✅ [0.02282747 0.97717255] 1 [0.68999577]\n",
      "✅ [0.09396149 0.90603846] 1 [0.48999578]\n",
      "❌ [0.50306726 0.49693277] 1 [0.28999576]\n",
      "❌ [0.9014637  0.09853638] 1 [0.08999576]\n",
      "✅ [0.97344774 0.02655223] 0 [0.]\n",
      "✅ [0.9875794 0.0124206] 0 [0.]\n",
      "✅ [0.9924866  0.00751343] 0 [0.]\n",
      "✅ [0.99441284 0.00558713] 0 [0.]\n",
      "✅ [0.99243814 0.00756192] 0 [0.]\n",
      "✅ [0.9876106  0.01238941] 0 [0.]\n",
      "✅ [0.9650555  0.03494444] 0 [0.]\n",
      "✅ [0.7782775  0.22172253] 0 [0.]\n",
      "✅ [0.2078043  0.79219574] 1 [1.929999]\n",
      "✅ [0.04283311 0.9571669 ] 1 [1.729999]\n",
      "✅ [0.02981052 0.97018945] 1 [1.529999]\n",
      "✅ [0.01753952 0.98246044] 1 [1.329999]\n",
      "✅ [0.01110288 0.98889714] 1 [1.1299989]\n",
      "✅ [0.01111274 0.98888725] 1 [0.929999]\n",
      "✅ [0.01351218 0.9864878 ] 1 [0.72999895]\n",
      "✅ [0.0202945 0.9797055] 1 [0.52999896]\n",
      "✅ [0.05248443 0.94751555] 1 [0.32999897]\n",
      "✅ [0.20090306 0.79909694] 1 [0.12999897]\n",
      "✅ [0.62105656 0.37894344] 0 [0.]\n",
      "✅ [0.9024798  0.09752013] 0 [0.]\n",
      "✅ [0.97181004 0.02818993] 0 [0.]\n",
      "✅ [0.9877507  0.01224934] 0 [0.]\n",
      "✅ [0.9936553  0.00634466] 0 [0.]\n",
      "✅ [0.9957926  0.00420732] 0 [0.]\n",
      "✅ [0.9955213  0.00447873] 0 [0.]\n",
      "✅ [0.994406   0.00559404] 0 [0.]\n",
      "✅ [0.9871761  0.01282393] 0 [0.]\n",
      "✅ [0.9685162  0.03148388] 0 [0.]\n",
      "✅ [0.85207146 0.14792849] 0 [0.]\n",
      "✅ [0.2620241 0.7379759] 1 [1.9399365]\n",
      "✅ [0.05652344 0.9434766 ] 1 [1.7399366]\n",
      "✅ [0.0282095  0.97179043] 1 [1.5399365]\n",
      "✅ [0.0168416  0.98315835] 1 [1.3399365]\n",
      "✅ [0.01306997 0.98693   ] 1 [1.1399366]\n",
      "✅ [0.01281537 0.9871846 ] 1 [0.9399365]\n",
      "✅ [0.01403623 0.9859637 ] 1 [0.73993653]\n",
      "✅ [0.02156223 0.9784378 ] 1 [0.53993654]\n",
      "✅ [0.04099374 0.95900625] 1 [0.33993655]\n",
      "✅ [0.12080412 0.87919587] 1 [0.13993654]\n",
      "❌ [0.40036124 0.59963876] 0 [0.]\n",
      "✅ [0.8246224 0.1753776] 0 [0.]\n",
      "✅ [0.9641862  0.03581379] 0 [0.]\n",
      "✅ [0.98701966 0.01298036] 0 [0.]\n",
      "✅ [0.9930648  0.00693523] 0 [0.]\n",
      "✅ [0.99586356 0.00413648] 0 [0.]\n",
      "✅ [0.9970368  0.00296323] 0 [0.]\n",
      "✅ [0.9954579  0.00454218] 0 [0.]\n",
      "✅ [0.9919075  0.00809252] 0 [0.]\n",
      "✅ [0.9763489  0.02365117] 0 [0.]\n",
      "✅ [0.8955552  0.10444481] 0 [0.]\n",
      "❌ [0.46952417 0.53047585] 0 [0.]\n",
      "✅ [0.0761933  0.92380667] 1 [1.8099539]\n",
      "✅ [0.02567594 0.9743241 ] 1 [1.6099539]\n",
      "✅ [0.0160918 0.9839081] 1 [1.409954]\n",
      "✅ [0.01214511 0.9878549 ] 1 [1.2099539]\n",
      "✅ [0.00996156 0.9900385 ] 1 [1.009954]\n",
      "✅ [0.01056108 0.98943895] 1 [0.8099539]\n",
      "✅ [0.0153383  0.98466176] 1 [0.60995394]\n",
      "✅ [0.03713509 0.9628649 ] 1 [0.40995392]\n",
      "✅ [0.15167917 0.84832084] 1 [0.20995393]\n",
      "✅ [0.49908587 0.50091416] 1 [0.00995393]\n",
      "✅ [0.85335547 0.14664456] 0 [0.]\n",
      "✅ [0.9537659  0.04623411] 0 [0.]\n",
      "✅ [0.9782104 0.0217896] 0 [0.]\n",
      "✅ [0.9888694  0.01113054] 0 [0.]\n",
      "✅ [0.9938443  0.00615576] 0 [0.]\n",
      "✅ [0.9962585  0.00374152] 0 [0.]\n",
      "✅ [0.99528056 0.00471946] 0 [0.]\n",
      "✅ [0.9946884  0.00531157] 0 [0.]\n",
      "✅ [0.98633575 0.01366426] 0 [0.]\n",
      "✅ [0.95208925 0.04791075] 0 [0.]\n",
      "✅ [0.7636716  0.23632838] 0 [0.]\n",
      "✅ [0.24657959 0.7534204 ] 1 [1.9600036]\n",
      "✅ [0.07214593 0.92785406] 1 [1.7600037]\n",
      "✅ [0.03564467 0.96435535] 1 [1.5600036]\n",
      "✅ [0.02437349 0.97562647] 1 [1.3600037]\n",
      "✅ [0.01818712 0.98181283] 1 [1.1600037]\n",
      "✅ [0.01756634 0.9824337 ] 1 [0.9600037]\n",
      "✅ [0.02139895 0.97860104] 1 [0.7600037]\n",
      "✅ [0.03541414 0.9645859 ] 1 [0.56000364]\n",
      "✅ [0.08352567 0.91647434] 1 [0.36000365]\n",
      "✅ [0.34663332 0.6533667 ] 1 [0.16000366]\n",
      "✅ [0.7817466 0.2182534] 0 [0.]\n",
      "✅ [0.95154923 0.04845072] 0 [0.]\n",
      "✅ [0.9822511  0.01774894] 0 [0.]\n",
      "✅ [0.99099416 0.00900583] 0 [0.]\n",
      "✅ [0.99442637 0.00557358] 0 [0.]\n",
      "✅ [0.9961514  0.00384854] 0 [0.]\n",
      "✅ [0.99757415 0.00242581] 0 [0.]\n",
      "✅ [0.9968719  0.00312817] 0 [0.]\n",
      "✅ [0.99625504 0.00374491] 0 [0.]\n",
      "✅ [0.9911862  0.00881378] 0 [0.]\n",
      "✅ [0.97313446 0.02686546] 0 [0.]\n",
      "✅ [0.8758442  0.12415584] 0 [0.]\n",
      "❌ [0.3572078 0.6427922] 0 [0.]\n",
      "✅ [0.06736603 0.932634  ] 1 [1.8000019]\n",
      "✅ [0.02892637 0.9710736 ] 1 [1.6000018]\n",
      "✅ [0.02023714 0.97976285] 1 [1.4000018]\n",
      "✅ [0.01393304 0.986067  ] 1 [1.2000018]\n",
      "✅ [0.01078396 0.98921597] 1 [1.0000018]\n",
      "✅ [0.01091015 0.9890899 ] 1 [0.8000018]\n",
      "✅ [0.01668434 0.98331565] 1 [0.6000018]\n",
      "✅ [0.0281572 0.9718428] 1 [0.40000182]\n",
      "✅ [0.080218   0.91978204] 1 [0.2000018]\n",
      "✅ [0.33393514 0.6660649 ] 1 [1.8119812e-06]\n",
      "✅ [0.8213077  0.17869231] 0 [0.]\n",
      "✅ [0.960433   0.03956702] 0 [0.]\n",
      "✅ [0.98421025 0.01578978] 0 [0.]\n",
      "✅ [0.9914569  0.00854303] 0 [0.]\n",
      "✅ [0.99503267 0.00496728] 0 [0.]\n",
      "✅ [0.9962914  0.00370863] 0 [0.]\n",
      "✅ [0.993853   0.00614708] 0 [0.]\n",
      "✅ [0.98959494 0.01040504] 0 [0.]\n",
      "✅ [0.95979595 0.04020406] 0 [0.]\n",
      "✅ [0.76164013 0.23835982] 0 [0.]\n",
      "✅ [0.21798448 0.78201544] 1 [1.8999879]\n",
      "✅ [0.05618762 0.9438124 ] 1 [1.6999879]\n",
      "✅ [0.02473718 0.9752628 ] 1 [1.4999878]\n",
      "✅ [0.01660389 0.9833961 ] 1 [1.2999879]\n",
      "✅ [0.01335646 0.9866435 ] 1 [1.0999879]\n",
      "✅ [0.01553903 0.984461  ] 1 [0.8999879]\n",
      "✅ [0.02486218 0.9751379 ] 1 [0.6999879]\n",
      "✅ [0.05955503 0.94044495] 1 [0.4999879]\n",
      "✅ [0.27435178 0.7256482 ] 1 [0.29998788]\n",
      "❌ [0.8094298  0.19057012] 1 [0.09998789]\n",
      "✅ [0.96845865 0.0315413 ] 0 [0.]\n",
      "✅ [0.9911911  0.00880893] 0 [0.]\n",
      "✅ [0.99588853 0.00411154] 0 [0.]\n",
      "✅ [0.9981255  0.00187446] 0 [0.]\n",
      "✅ [9.9903512e-01 9.6491573e-04] 0 [0.]\n",
      "✅ [9.9921143e-01 7.8857364e-04] 0 [0.]\n",
      "✅ [0.9989988  0.00100117] 0 [0.]\n",
      "✅ [0.9988771  0.00112286] 0 [0.]\n",
      "✅ [0.99827266 0.00172729] 0 [0.]\n",
      "✅ [0.9974963  0.00250365] 0 [0.]\n",
      "✅ [0.99625444 0.0037456 ] 0 [0.]\n",
      "✅ [0.99419194 0.00580805] 0 [0.]\n",
      "✅ [0.9907883  0.00921169] 0 [0.]\n",
      "✅ [0.98043936 0.01956062] 0 [0.]\n",
      "✅ [0.8875102  0.11248975] 0 [0.]\n",
      "✅ [0.3710592  0.62894076] 1 [1.8499553]\n",
      "✅ [0.06044484 0.93955517] 1 [1.6499553]\n",
      "✅ [0.0305724 0.9694276] 1 [1.4499552]\n",
      "✅ [0.01807011 0.98192984] 1 [1.2499553]\n",
      "✅ [0.01237726 0.98762274] 1 [1.0499552]\n",
      "✅ [0.01028716 0.98971283] 1 [0.84995526]\n",
      "✅ [0.01108994 0.9889101 ] 1 [0.6499553]\n",
      "✅ [0.02505825 0.9749417 ] 1 [0.44995528]\n",
      "✅ [0.09352696 0.906473  ] 1 [0.24995527]\n",
      "✅ [0.45824456 0.54175544] 1 [0.04995527]\n",
      "✅ [0.84261703 0.15738298] 0 [0.]\n",
      "✅ [0.94291943 0.05708064] 0 [0.]\n",
      "✅ [0.975759   0.02424101] 0 [0.]\n",
      "✅ [0.9895443  0.01045577] 0 [0.]\n",
      "✅ [0.99425805 0.0057419 ] 0 [0.]\n",
      "✅ [0.9962878  0.00371212] 0 [0.]\n",
      "✅ [0.9956857  0.00431434] 0 [0.]\n",
      "✅ [0.9942983  0.00570176] 0 [0.]\n",
      "✅ [0.98745584 0.01254421] 0 [0.]\n",
      "❌ [0.95177007 0.04822998] 1 [1.9499453]\n",
      "❌ [0.70660853 0.29339147] 1 [1.7499454]\n",
      "✅ [0.16587922 0.8341208 ] 1 [1.5499454]\n",
      "✅ [0.05802201 0.941978  ] 1 [1.3499453]\n",
      "✅ [0.03226021 0.9677398 ] 1 [1.1499454]\n",
      "✅ [0.02033151 0.97966844] 1 [0.94994533]\n",
      "✅ [0.01265789 0.9873422 ] 1 [0.74994534]\n",
      "✅ [0.01092229 0.9890777 ] 1 [0.54994535]\n",
      "✅ [0.01406293 0.98593706] 1 [0.34994537]\n",
      "✅ [0.02077274 0.97922724] 1 [0.14994535]\n",
      "❌ [0.06351669 0.93648326] 0 [0.]\n",
      "❌ [0.25103673 0.74896324] 0 [0.]\n",
      "✅ [0.69289374 0.30710623] 0 [0.]\n",
      "✅ [0.9158843  0.08411566] 0 [0.]\n",
      "✅ [0.9675733  0.03242666] 0 [0.]\n",
      "✅ [0.98066986 0.01933015] 0 [0.]\n",
      "✅ [0.98934245 0.01065752] 0 [0.]\n",
      "✅ [0.9942781  0.00572184] 0 [0.]\n",
      "✅ [0.99459237 0.00540761] 0 [0.]\n",
      "✅ [0.99288636 0.00711365] 0 [0.]\n",
      "✅ [0.98639524 0.01360475] 0 [0.]\n",
      "✅ [0.94507027 0.05492971] 0 [0.]\n",
      "❌ [0.6370742  0.36292586] 1 [1.9699955]\n",
      "✅ [0.152549 0.847451] 1 [1.7699955]\n",
      "✅ [0.04484966 0.95515037] 1 [1.5699955]\n",
      "✅ [0.02086703 0.97913295] 1 [1.3699955]\n",
      "✅ [0.01453698 0.985463  ] 1 [1.1699955]\n",
      "✅ [0.01103916 0.9889608 ] 1 [0.9699955]\n",
      "✅ [0.00983812 0.9901619 ] 1 [0.7699955]\n",
      "✅ [0.00940516 0.99059486] 1 [0.5699955]\n",
      "✅ [0.01482149 0.98517853] 1 [0.3699955]\n",
      "✅ [0.04707849 0.9529215 ] 1 [0.1699955]\n",
      "❌ [0.1474848 0.8525152] 0 [0.]\n",
      "❌ [0.43684188 0.5631581 ] 0 [0.]\n",
      "✅ [0.8285485  0.17145152] 0 [0.]\n",
      "✅ [0.956451   0.04354903] 0 [0.]\n",
      "✅ [0.98195535 0.0180446 ] 0 [0.]\n",
      "✅ [0.9909314 0.0090686] 0 [0.]\n",
      "✅ [0.9952545  0.00474555] 0 [0.]\n",
      "✅ [0.99694806 0.00305195] 0 [0.]\n",
      "✅ [0.994756   0.00524397] 0 [0.]\n",
      "✅ [0.99339336 0.00660669] 0 [0.]\n",
      "✅ [0.9866498 0.0133502] 0 [0.]\n",
      "✅ [0.9673228  0.03267713] 0 [0.]\n",
      "❌ [0.822589   0.17741105] 1 [1.8400348]\n",
      "✅ [0.32909608 0.6709039 ] 1 [1.6400348]\n",
      "✅ [0.12361946 0.87638056] 1 [1.4400349]\n",
      "✅ [0.06186331 0.9381367 ] 1 [1.2400348]\n",
      "✅ [0.03100864 0.9689914 ] 1 [1.0400348]\n",
      "✅ [0.01462317 0.9853769 ] 1 [0.84003484]\n",
      "✅ [0.00842895 0.99157107] 1 [0.6400348]\n",
      "✅ [0.00653131 0.99346864] 1 [0.4400348]\n",
      "✅ [0.00553395 0.9944661 ] 1 [0.24003482]\n",
      "✅ [0.00706629 0.99293375] 1 [0.04003482]\n",
      "❌ [0.01901267 0.9809873 ] 0 [0.]\n",
      "❌ [0.0977321  0.90226793] 0 [0.]\n",
      "❌ [0.36350372 0.6364963 ] 0 [0.]\n",
      "✅ [0.702857   0.29714298] 0 [0.]\n",
      "✅ [0.91448605 0.08551396] 0 [0.]\n",
      "✅ [0.9778521  0.02214789] 0 [0.]\n",
      "✅ [0.9931479  0.00685202] 0 [0.]\n",
      "✅ [0.9957898  0.00421025] 0 [0.]\n",
      "✅ [0.9948441  0.00515595] 0 [0.]\n",
      "✅ [0.98543555 0.01456443] 0 [0.]\n",
      "✅ [0.9232308  0.07676921] 0 [0.]\n",
      "✅ [0.43585408 0.564146  ] 1 [1.8000025]\n",
      "✅ [0.07084098 0.929159  ] 1 [1.6000025]\n",
      "✅ [0.04315022 0.95684975] 1 [1.4000025]\n",
      "✅ [0.02521876 0.9747813 ] 1 [1.2000024]\n",
      "✅ [0.01629283 0.98370713] 1 [1.0000025]\n",
      "✅ [0.01249356 0.9875064 ] 1 [0.80000246]\n",
      "✅ [0.0108953  0.98910475] 1 [0.60000247]\n",
      "✅ [0.01466293 0.9853371 ] 1 [0.40000248]\n",
      "✅ [0.03435795 0.9656421 ] 1 [0.20000248]\n",
      "✅ [0.12768953 0.8723104 ] 1 [2.4795531e-06]\n",
      "❌ [0.44575122 0.5542488 ] 0 [0.]\n",
      "✅ [0.8073105  0.19268943] 0 [0.]\n",
      "✅ [0.9249154 0.0750846] 0 [0.]\n",
      "✅ [0.97267556 0.02732438] 0 [0.]\n",
      "✅ [0.99082047 0.00917952] 0 [0.]\n",
      "✅ [0.9966461  0.00335393] 0 [0.]\n",
      "✅ [0.99671555 0.00328442] 0 [0.]\n",
      "✅ [0.995876   0.00412407] 0 [0.]\n",
      "✅ [0.9904931  0.00950686] 0 [0.]\n",
      "✅ [0.9605498  0.03945022] 0 [0.]\n",
      "✅ [0.74222755 0.25777245] 0 [0.]\n",
      "✅ [0.19321682 0.8067832 ] 1 [1.9399996]\n",
      "✅ [0.04252412 0.9574759 ] 1 [1.7399995]\n",
      "✅ [0.01949487 0.98050517] 1 [1.5399995]\n",
      "✅ [0.0115552 0.9884448] 1 [1.3399996]\n",
      "✅ [0.00949896 0.99050105] 1 [1.1399995]\n",
      "✅ [0.00860711 0.99139285] 1 [0.9399995]\n",
      "✅ [0.01107951 0.9889205 ] 1 [0.73999953]\n",
      "✅ [0.01809575 0.9819042 ] 1 [0.53999954]\n",
      "✅ [0.06580303 0.934197  ] 1 [0.33999953]\n",
      "✅ [0.31585765 0.68414235] 1 [0.13999954]\n",
      "✅ [0.73927087 0.26072913] 0 [0.]\n",
      "✅ [0.9239267  0.07607327] 0 [0.]\n",
      "✅ [0.970902   0.02909798] 0 [0.]\n",
      "✅ [0.98633367 0.01366638] 0 [0.]\n",
      "✅ [0.9930248  0.00697522] 0 [0.]\n",
      "✅ [0.9955232  0.00447675] 0 [0.]\n",
      "✅ [0.99539536 0.00460461] 0 [0.]\n",
      "✅ [0.9903681  0.00963187] 0 [0.]\n",
      "✅ [0.96594405 0.03405588] 0 [0.]\n",
      "✅ [0.82524395 0.174756  ] 0 [0.]\n",
      "✅ [0.2981041 0.7018959] 1 [1.91995]\n",
      "✅ [0.04782381 0.9521763 ] 1 [1.71995]\n",
      "✅ [0.0192711 0.9807289] 1 [1.5199499]\n",
      "✅ [0.01235124 0.9876488 ] 1 [1.31995]\n",
      "✅ [0.01064519 0.98935485] 1 [1.1199499]\n",
      "✅ [0.00896444 0.9910356 ] 1 [0.91994995]\n",
      "✅ [0.00819967 0.99180037] 1 [0.71994996]\n",
      "✅ [0.01095476 0.98904526] 1 [0.51995]\n",
      "✅ [0.03405173 0.9659483 ] 1 [0.31994995]\n",
      "✅ [0.1435264  0.85647357] 1 [0.11994996]\n",
      "✅ [0.5681744  0.43182555] 0 [0.]\n",
      "✅ [0.90641844 0.09358153] 0 [0.]\n",
      "✅ [0.9793097  0.02069029] 0 [0.]\n",
      "✅ [0.991366   0.00863399] 0 [0.]\n",
      "✅ [0.9955338  0.00446616] 0 [0.]\n",
      "✅ [0.99710554 0.00289445] 0 [0.]\n",
      "✅ [0.9970843  0.00291572] 0 [0.]\n",
      "✅ [0.99670225 0.00329774] 0 [0.]\n",
      "✅ [0.99441373 0.00558626] 0 [0.]\n",
      "✅ [0.98685    0.01315005] 0 [0.]\n",
      "✅ [0.9608628  0.03913717] 0 [0.]\n",
      "❌ [0.8191443  0.18085563] 1 [1.8699379]\n",
      "✅ [0.2538623 0.7461377] 1 [1.6699378]\n",
      "✅ [0.05212476 0.94787526] 1 [1.4699379]\n",
      "✅ [0.02212612 0.97787386] 1 [1.2699379]\n",
      "✅ [0.0173599 0.9826401] 1 [1.069938]\n",
      "✅ [0.01474572 0.98525435] 1 [0.8699379]\n",
      "✅ [0.01206509 0.98793495] 1 [0.6699379]\n",
      "✅ [0.01136482 0.9886352 ] 1 [0.4699379]\n",
      "✅ [0.01402561 0.9859744 ] 1 [0.2699379]\n",
      "✅ [0.02714694 0.972853  ] 1 [0.0699379]\n",
      "❌ [0.10439557 0.89560443] 0 [0.]\n",
      "❌ [0.47074142 0.52925855] 0 [0.]\n",
      "✅ [0.857071   0.14292905] 0 [0.]\n",
      "✅ [0.9590162  0.04098381] 0 [0.]\n",
      "✅ [0.984411   0.01558898] 0 [0.]\n",
      "✅ [0.99305785 0.0069422 ] 0 [0.]\n",
      "✅ [0.9960472 0.0039528] 0 [0.]\n",
      "✅ [0.9969561  0.00304388] 0 [0.]\n",
      "✅ [0.99658245 0.00341755] 0 [0.]\n",
      "✅ [0.99244976 0.00755032] 0 [0.]\n",
      "✅ [0.97522503 0.02477493] 0 [0.]\n",
      "✅ [0.84956104 0.15043896] 0 [0.]\n",
      "✅ [0.25621608 0.74378395] 1 [1.9399854]\n",
      "✅ [0.03741866 0.9625814 ] 1 [1.7399853]\n",
      "✅ [0.02172109 0.97827893] 1 [1.5399854]\n",
      "✅ [0.01166629 0.9883337 ] 1 [1.3399854]\n",
      "✅ [0.00848017 0.99151987] 1 [1.1399853]\n",
      "✅ [0.00713598 0.992864  ] 1 [0.9399854]\n",
      "✅ [0.00672462 0.9932754 ] 1 [0.73998535]\n",
      "✅ [0.00856514 0.9914349 ] 1 [0.53998536]\n",
      "✅ [0.02308965 0.97691035] 1 [0.33998537]\n",
      "✅ [0.11289371 0.88710624] 1 [0.13998537]\n",
      "✅ [0.5227048  0.47729525] 0 [0.]\n",
      "✅ [0.8691886  0.13081138] 0 [0.]\n",
      "✅ [0.96692085 0.03307918] 0 [0.]\n",
      "✅ [0.98822206 0.01177788] 0 [0.]\n",
      "✅ [0.9947299 0.0052701] 0 [0.]\n",
      "✅ [0.9971758  0.00282422] 0 [0.]\n",
      "✅ [0.99612063 0.00387936] 0 [0.]\n",
      "✅ [0.99331665 0.00668328] 0 [0.]\n",
      "✅ [0.9805807  0.01941933] 0 [0.]\n",
      "✅ [0.92245203 0.07754797] 0 [0.]\n",
      "✅ [0.4754647 0.5245353] 1 [1.9399854]\n",
      "✅ [0.06190514 0.9380948 ] 1 [1.7399853]\n",
      "✅ [0.02671936 0.9732806 ] 1 [1.5399854]\n",
      "✅ [0.014258 0.985742] 1 [1.3399854]\n",
      "✅ [0.00988594 0.9901141 ] 1 [1.1399853]\n",
      "✅ [0.00715081 0.99284923] 1 [0.9399854]\n",
      "✅ [0.00571169 0.9942883 ] 1 [0.73998535]\n",
      "✅ [0.00803546 0.9919646 ] 1 [0.53998536]\n",
      "✅ [0.01461138 0.9853886 ] 1 [0.33998537]\n",
      "✅ [0.05148009 0.94851995] 1 [0.13998537]\n",
      "❌ [0.23028533 0.7697147 ] 0 [0.]\n",
      "✅ [0.6108194 0.3891806] 0 [0.]\n",
      "✅ [0.8524807  0.14751928] 0 [0.]\n",
      "✅ [0.94076884 0.05923115] 0 [0.]\n",
      "✅ [0.9751277  0.02487231] 0 [0.]\n",
      "✅ [0.9895712  0.01042879] 0 [0.]\n",
      "✅ [0.99545324 0.00454678] 0 [0.]\n",
      "✅ [0.9955383  0.00446171] 0 [0.]\n",
      "✅ [0.9934999  0.00650016] 0 [0.]\n",
      "✅ [0.97965735 0.02034267] 0 [0.]\n",
      "✅ [0.92698777 0.07301228] 0 [0.]\n",
      "✅ [0.74842334 0.2515767 ] 0 [0.]\n",
      "✅ [0.26878503 0.731215  ] 1 [1.8900087]\n",
      "✅ [0.07633513 0.92366487] 1 [1.6900086]\n",
      "✅ [0.04129624 0.95870376] 1 [1.4900086]\n",
      "✅ [0.03297675 0.9670233 ] 1 [1.2900087]\n",
      "✅ [0.02123081 0.9787692 ] 1 [1.0900086]\n",
      "✅ [0.01430538 0.9856946 ] 1 [0.8900086]\n",
      "✅ [0.01060287 0.98939705] 1 [0.69000864]\n",
      "✅ [0.01193865 0.9880613 ] 1 [0.49000865]\n",
      "✅ [0.02056375 0.9794362 ] 1 [0.29000863]\n",
      "✅ [0.05819478 0.9418052 ] 1 [0.09000864]\n",
      "❌ [0.27407026 0.72592974] 0 [0.]\n",
      "✅ [0.7034646  0.29653537] 0 [0.]\n",
      "✅ [0.9115983  0.08840165] 0 [0.]\n",
      "✅ [0.9712172 0.0287828] 0 [0.]\n",
      "✅ [0.988576   0.01142402] 0 [0.]\n",
      "✅ [0.9950754  0.00492456] 0 [0.]\n",
      "✅ [0.99705493 0.00294501] 0 [0.]\n",
      "✅ [0.99608374 0.0039163 ] 0 [0.]\n",
      "✅ [0.9914226  0.00857745] 0 [0.]\n",
      "✅ [0.97449684 0.02550322] 0 [0.]\n",
      "✅ [0.90286255 0.09713738] 0 [0.]\n",
      "✅ [0.5945981 0.4054019] 0 [0.]\n",
      "✅ [0.14366622 0.8563338 ] 1 [1.8699467]\n",
      "✅ [0.05521403 0.94478595] 1 [1.6699468]\n",
      "✅ [0.02729751 0.9727025 ] 1 [1.4699467]\n",
      "✅ [0.01619394 0.983806  ] 1 [1.2699468]\n",
      "✅ [0.01033794 0.98966205] 1 [1.0699468]\n",
      "✅ [0.00898488 0.99101514] 1 [0.8699468]\n",
      "✅ [0.00885483 0.9911452 ] 1 [0.6699468]\n",
      "✅ [0.01297273 0.98702735] 1 [0.46994677]\n",
      "✅ [0.02264071 0.97735924] 1 [0.26994675]\n",
      "✅ [0.06904853 0.9309515 ] 1 [0.06994677]\n",
      "❌ [0.29059345 0.70940655] 0 [0.]\n",
      "✅ [0.6518711  0.34812897] 0 [0.]\n",
      "✅ [0.8623354 0.1376646] 0 [0.]\n",
      "✅ [0.9558956 0.0441044] 0 [0.]\n",
      "✅ [0.9861334  0.01386658] 0 [0.]\n",
      "✅ [0.9951473  0.00485269] 0 [0.]\n",
      "✅ [0.9982029  0.00179705] 0 [0.]\n",
      "✅ [0.998489   0.00151096] 0 [0.]\n",
      "✅ [0.99895835 0.00104158] 0 [0.]\n",
      "✅ [0.99865    0.00134999] 0 [0.]\n",
      "✅ [0.99799156 0.00200844] 0 [0.]\n",
      "✅ [0.99723226 0.00276771] 0 [0.]\n",
      "✅ [0.9970943  0.00290577] 0 [0.]\n",
      "✅ [0.99629444 0.00370552] 0 [0.]\n",
      "✅ [0.99551165 0.00448843] 0 [0.]\n",
      "Test loss: 0.3888820848575463, Test accuracy: 93.76114081996435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52082/4060731017.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
      "/tmp/ipykernel_52082/1068058261.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('checkpoint.pth')\n",
      "/home/thomas/Documents/ActiveAlarm_Processing/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "testuuid = \"94ABBF79-C4A7-4715-A661-94BC30627CBD\"\n",
    "d_test, t_test = id.import_data(testuuid, 0.2, 100, 2)\n",
    "t_test_original = copy.deepcopy(t_test)\n",
    "t_test[t_test != 0] = 1\n",
    "t_test = np.sum(t_test, axis=1)\n",
    "t_test = np.squeeze(t_test, axis=-1)\n",
    "x_test = torch.tensor(d_test, dtype=torch.float32).to(device)\n",
    "test_dataset = SitUpDataset(x_test, t_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "\n",
    "# d_test, t_test = id.import_data(testuuid, 0.02)\n",
    "\n",
    "# t_test_original = copy.deepcopy(t_test)\n",
    "\n",
    "# t_test[t_test != 0] = 1\n",
    "\n",
    "# t_test = np.sum(t_test, axis=1)\n",
    "# print(t_final)\n",
    "# t_test = np.squeeze(t_test, axis=-1)\n",
    "# print(t_final)\n",
    "# # indices = list(range(len(d_test)))\n",
    "# # sampled_indices = random.sample(indices, 1000)\n",
    "# # sampled_indices = sorted(sampled_indices)\n",
    "# # d_test, t_test = d_test[sampled_indices], t_test[sampled_indices]\n",
    "\n",
    "# x_test = torch.tensor(d_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# # if len(t_test[0]) == num_classes:\n",
    "# #     y_test = torch.tensor(t_test, dtype=torch.float32).to(device)\n",
    "# # else:\n",
    "# #     if len(t_test[0]) < num_classes:\n",
    "# #         print(\"lengthening test results\")\n",
    "# #         difference = num_classes - len(t_test[0])\n",
    "# #         lengthened_t_test = []\n",
    "# #         for i, t in enumerate(t_test):\n",
    "# #             zeros = np.zeros((difference,1))\n",
    "# #             print(t, zeros)\n",
    "# #             lengthened_t_final.append(np.concatenate((t, np.zeros((difference,1))), axis=0))\n",
    "# #         print(lengthened_t_test)\n",
    "# #         y_test = torch.tensor(lengthened_t_test, dtype=torch.float32).to(device)\n",
    "# #     else:\n",
    "# #         # This could be relaxed and loss calculations changed if this is annoying\n",
    "# #         raise RuntimeError(\"Test outputs must be the same shape or smaller than what the model was trained on\")\n",
    "\n",
    "# test_dataset = SitUpDataset(x_test, t_test)\n",
    "# print(test_dataset.targets)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "        \n",
    "# Load the saved model\n",
    "model = SitUpModel(num_features, num_classes).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "total_test_loss = 0.0\n",
    "total_test_correct = 0\n",
    "batch_num = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        # Load the batch and conver to the correct datatypes\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.long().to(device)\n",
    "\n",
    "        # Calculate batch outputs and loss\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Calculate training stats\n",
    "        total_test_loss += loss.item() * x_batch.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_test_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        # Convert the outputs to a NumPy array\n",
    "        outputs = outputs.cpu().numpy()\n",
    "\n",
    "        for i, output in enumerate(outputs):\n",
    "            output_num_situps = np.argmax(output)\n",
    "            true_num_situps = y_batch[i]\n",
    "            if true_num_situps == output_num_situps:\n",
    "                verify_mark = \"✅\"\n",
    "            else:\n",
    "                verify_mark = \"❌\"\n",
    "            print(verify_mark, output, y_batch[i].item(), t_test_original[batch_num*64 + i].flatten())\n",
    "        batch_num += 1\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_dataset)\n",
    "test_accuracy = 100* total_test_correct / len(test_dataset)\n",
    "print(f\"Test loss: {avg_test_loss}, Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# # Run the model on the new dataset\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(test_data)\n",
    "#     total_test_loss = loss.item() * x_batch.size(0)\n",
    "#     _, predicted = torch.max(outputs, 1)\n",
    "#     total_eval_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52082/3400682453.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('checkpoint.pth')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     26\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader\u001b[49m:\n\u001b[1;32m     28\u001b[0m     x_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "\n",
    "# Reinitialize the model and optimizer\n",
    "model = SitUpModel(num_features=num_features, num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Load the state dictionaries\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "# Retrieve the last epoch\n",
    "start_epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "# Define the number of additional epochs\n",
    "additional_epochs = 300\n",
    "total_epochs = start_epoch + additional_epochs\n",
    "\n",
    "# Training loop  # Initial number of epochs\n",
    "for epoch in range(start_epoch, total_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        x_batch = x_batch.permute(0, 1, 2).to(device)\n",
    "        y_batch = y_batch.long().to(device)\n",
    "        # mask_batch = mask_batch.to(device)\n",
    "\n",
    "        outputs = model(x_batch)\n",
    "        # print(outputs)\n",
    "        # print(y_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        total_loss += loss.item() * x_batch.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        \n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    accuracy = 100* correct / len(dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%, learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        # print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# for epoch in range(start_epoch, total_epochs):\n",
    "#     model.train()\n",
    "#     for x_batch, y_batch in dataloader:\n",
    "#         x_batch = x_batch.permute(0, 1, 2).to(device)\n",
    "#         y_batch = y_batch.to(device)\n",
    "#         # mask_batch = mask_batch.to(device)\n",
    "\n",
    "#         predictions = model(x_batch)\n",
    "#         loss = mse_loss(predictions, y_batch)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     scheduler.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{total_epochs}, Loss: {loss.item():.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# Save the model again after additional training\n",
    "torch.save({\n",
    "    'epoch': total_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'loss': loss.item(),\n",
    "}, 'checkpoint.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
